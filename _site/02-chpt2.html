<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(4)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(4) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(4) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(4) > ul.nav-list { display: block; } </style> <script src="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/assets/js/vendor/lunr.min.js"></script> <script src="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Chapter 2 - The ELBO Paradigm — Proxy Objective for True Data Maximization | PrgM2 /pR’gem/2</title> <meta name="generator" content="Jekyll v3.10.0" /> <meta property="og:title" content="Chapter 2 - The ELBO Paradigm — Proxy Objective for True Data Maximization" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="A preliminary mathematical exegesis of diffusion models" /> <meta property="og:description" content="A preliminary mathematical exegesis of diffusion models" /> <link rel="canonical" href="http://localhost:4000/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/02-chpt2.html" /> <meta property="og:url" content="http://localhost:4000/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/02-chpt2.html" /> <meta property="og:site_name" content="PrgM2 /pR’gem/2" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Chapter 2 - The ELBO Paradigm — Proxy Objective for True Data Maximization" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"A preliminary mathematical exegesis of diffusion models","headline":"Chapter 2 - The ELBO Paradigm — Proxy Objective for True Data Maximization","url":"http://localhost:4000/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/02-chpt2.html"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/" class="site-title lh-tight"> PrgM2 /pR'gem/2 </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/" class="nav-list-link">Cover Page</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/00-chpt0.html" class="nav-list-link">Chapter 0 - Preface</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/01-chpt1.html" class="nav-list-link">Chapter 1 - The High-Dimensional Structure of True Data</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/02-chpt2.html" class="nav-list-link">Chapter 2 - The ELBO Paradigm --- Proxy Objective for True Data Maximization</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/03-chpt3.html" class="nav-list-link">Chapter 3 - Dissecting the ELBO - Diffusion Models as Distribution-Transitioned Dynamics</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/04-chpt4.html" class="nav-list-link">Chapter 4 - Implementation on machine - Get our hands dirty</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/appendix-DDPM.html" class="nav-list-link">Appendix - Code for DDPM</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/resume.html" class="nav-list-link">About Author / Cite this</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/sponsor.html" class="nav-list-link">Buy me coffee(s)</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search PrgM2 /pR'gem/2" aria-label="Search PrgM2 /pR'gem/2" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <head> <title>A Preliminary Mathematical Exegesis of Diffusion Models</title> <meta name="description" content="An in-depth mathematical analysis of diffusion models in machine learning." /> <meta name="keywords" content="diffusion models, mathematics, machine learning, exegesis" /> <meta name="author" content="Shuyue Wang" /> <!-- Open Graph (for social sharing) --> <meta property="og:title" content="A Preliminary Mathematical Exegesis of Diffusion Models" /> <meta property="og:description" content="An in-depth mathematical analysis of diffusion models in machine learning." /> <meta property="og:url" content="https://shuyuew1991.github.io/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/" /> <meta property="og:type" content="website" /> </head> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async=""></script> <div style="text-align: left; font-size: 1.3em;"> Chapter 2 - The ELBO Paradigm --- Proxy Objective for True Data Maximization </div> <p><br /></p> <div style="text-align: center;"> <img src="./assets/images/fig_ch02.png" style="width: 45%; max-width: 400px; height: auto; margin: 0 auto;" /> </div> <p>In previous chapter, we talked about finding the best modeled distribution via evaluating the likelihood \(p(x)\)  of the <strong>observed</strong> data, because that measures how well they explain the observation - higher likelihood suggests a better model.</p> <p>Question now is: how to maximize the \(p(x)\), i.e. optimize \(p(x)\) to achieve maximal likelihood?</p> <p>In this chapter, to get this game going, we will introduce two <em>deus ex machina</em> magics, the first one being</p> <blockquote> <p>the latent variables, which is often denoted as \(z\).</p> </blockquote> <p>The incorporation of latent variables in generative models represents a powerful paradigm grounded in critical insights about data structure. Real-world datasets often exhibit complex dependencies of <strong>underlying, unobserved</strong> factors.</p> <blockquote> <p>For instance, image data contains implicit attributes like illumination geometry and object orientation that are not explicitly encoded in pixel values but significantly influence the observed patterns.</p> </blockquote> <p>By explicitly representing these hidden factors, latent variables enable models to capture richer data structure. Without latent variables, we would face the formidable challenge of modeling complex high-dimensional distributions directly - such as capturing intricate pixel-level correlations in images - which is both computationally intractable and statistically inefficient. Latent variables decompose the problem into more manageable components:</p> <ul> <li>a simple <em>latent space distribution \(p(z)\)</em>, and</li> <li>a <em>conditional data distribution \(p(x|z)\)</em>,</li> </ul> <p>where \(z\) represents the latent factors and \(x\) the observed data. This framework offers a structured, interpretable, and scalable approach—transforming an intractable problem into one where hidden factors systematically explain observed phenomena.</p> <p>The metaphor of Plato’s Cave from <em>The Republic</em> provides a powerful analogy for understanding latent variables in generative models.</p> <blockquote> <p>In the allegory, prisoners are chained in a cave, seeing only shadows cast on a wall by objects they cannot directly observe.</p> </blockquote> <p>This mirrors the relationship between observed data and latent variables. The shadows are like our raw data (e.g., pixel values in images), mere surface-level projections. The true forms are the latent variables—the unobserved, higher-dimensional factors.</p> <blockquote> <p>e.g., lighting, pose, or semantic meaning that <em>generate</em> the data, just as the objects outside the cave cast the shadows.</p> </blockquote> <p>Reality is richer than what we directly perceive.</p> <p><strong>We now try maximizing \(p(x)\) by utilizing latents \(z\).</strong></p><hr /> <p>We’re here to find the distribution in which the observed data \(x_1, x_2, …, x_n\) would be highest in probability compared with other distributions. In modeling terms, this translates to finding a configuration of model parameters  such that the observed data have the highest probability compared with other configuration.</p> <p>Keep in mind that it has never been a problem of probability value arithmetic, as is suggested by its denotation format; it is a <strong>distribution</strong> (i.e. the best \(p\), in whichever form it takes, that we are actually looking for).</p> <p>Inspired the above manageable components \(p(z)\) and \(p(x|z)\), one way of linking latents \(z\) to \(p(x)\) is to think about:</p> \[p(x)=\int{p_\theta(x,z)dz}=\int{p_\theta(x|z)p(z)dz}\] <p>where \(p_\theta(x,z)\) refers to a new modeled (hence the \(_\theta\)) probability distribution (hence the \(p\)) with new set of params (hence the \(x,z\) ).</p> <p>The decomposition of the integrand is directly from the chain of rule in probability.</p> <ul> <li>\(p(x|z)\), i.e. <em>decoder</em>, describes how observations \(x\) are generated from latents \(z\);</li> <li>\(p(z|x)\), i.e. <em>encoder</em>, describes how latents \(z\) can be inferred from observations.</li> </ul> <p>Note that the equation holds regardless of whether \(x\) and \(z\) are independent: independence would imply \(p(x,z)=p(x)p(z)\), but this is not necessary for the marginalization to be valid.</p> <p>With this, one might propose the procedures of estimate \(p(x\)):</p> <ul> <li>sample  \(z\) from \(p(z)\)</li> <li>for a given \(\theta\), calculate \(p_\theta(x|z)\) , i.e. the probability of observing the data \(x\) we have observed given a sampled \(z\).</li> <li>do this with sufficient times of \(z\) sampling, thus get to compute the integral.</li> <li>Therefore, by adjusting the model param \(\theta\), the integral gets bigger or smaller correspondingly, until the \(\theta\) corresponds to one high value of \(p(x)\) is found.</li> </ul> <p>However, the intractability of the integration is a major challenge itself.</p> <p>\(z\) is typically a high-dimensional vector. The integral is over all possible values of \(z\), which is computationally infeasible for even moderate size.</p> <blockquote> <p>For example: if \(z\) has 100 dimensions and we discretize each dimension into just 10 values, the number of terms in the sum grows as much as \(10^{100}\). The decoder \(p_\theta(x|z)\) , like other mathematical things in machine learning that is beyond explicit expression, is usually modeled by a neural network. To such a complex nonlinear function, there’s no analytical formula for integrating it over \(z\), even if \(p(z)\) is as simple as Gaussian. Suppose we repeat \(N\) times to yield the approximation:</p> </blockquote> \[p_{\boldsymbol{\theta}}(x) = \int p_{\theta}(x|z)p(z)dz \approx \frac{1}{N}\sum_{i=1}^N p_{\theta}(x|z^{(i)})\] <p>This approximation converges to the true expectation as \(N \to \infty\) by the law of large numbers. We optimize parameters \(\theta\) to maximize \(p_{\theta}(x)\), thereby improving the model’s fit to the observed data while maintaining the learned low-dimensional structure. However, it takes very large number to await the valid \(N\) to come. In addition, most \(z\) samples obtained in this approach will contribute negligibly to \(p_\theta(x)\), since \(p(z)\) is uninformed about \(x\), so most \(z\) values will lead to \(p_\theta(x∣z)≈0\), just consider how much naturalimages occupies the whole space. Blindly sampling  \(z \sim p(z)\) provides little guidance to the decoder about which regions of \(z\)-space are relevant for generating meaningful \(x\).</p><hr /> <p>A second approach of linking latents \(z\) to \(p(x)\) is through chain rule of probability:</p> \[p(x)=\frac{p(x,z)}{p(z|x)}\] <p>Since the latents \(z\) are in both numerator and denominators, no direct observations can be made. Given our goal is to maximize \(p(x)\), we’re here faced with the problem of maximizing two unknown functions distributions simultaneously.</p> <p>The denominator can be modeled as encoder network, and it can be denoted as \(p_\phi(z|x)\). It is quite instinctive to imagine a mapping from observed data \(x\) towards latents \(z\). But in mainstream literature, the \(p_\phi(z|x)\) here is written as \(q_\phi(z|x)\), a notation that emphasizes it’s an <em>approximation</em> of the true posterior \(p(z|x)\).</p> <p>Modeling the numerator  \(p_\theta(x,z)\) might seem conceptually straightforward: we could simply introduce another parameterized distribution. But in practice, designing a network that simultaneously takes both observed data \(x\) and latent variables \(z\) as inputs presents significant architectural challenges, making this approach less intuitive to implement. We consider use chain rule of probability again to decompose \(p_\theta(x,z)\) into \(p(z)p_\theta(x|z)\). Analogously, the \(p_\theta(x|z)\) can be modeled as a decoder network.</p> <p>Now with the two networks are baptized, there’s the term of \(p(z)\) left.</p> <!-- That dictates our prior to be $$p(z) \sim \mathcal{N}(\mu, \sigma^2).$$ --> <p>To wrap up our second approach, the new way to estimate \(p(x)\) can be like this:</p> <ul> <li>sample one \(z_i\) (\(i\) being arbitrary integer as sampling index) from the variational posterior \(q_\phi(z|x)\) by inputting one observed data \(x_i\) into the network.</li> <li>one data point in true data dataset can be sampled more than one time.</li> <li>with the \(z_i\), evaluate \(p(x)\) as \(\frac{p(z_i)p_\theta(x|z_i)}{q_\phi(z_i|x)}\).</li> </ul> <p>An expectation (even approximated with a few samples) gives a smoother, more stable gradient, because averaging over multiple samples would reduce variance. So let there be multiple samples instead of only one, and average the sum of their values in the final \(p(x)\)’s, which is basically the idea of Monte Carlo estimating.</p> <p>Wait，there is another concern is from \(q_\phi(z|x)\). All denominators are headaches for numerical computation.</p> <blockquote> <p>It may lead to high variance if that is a poor approximation of the true encoder, which is a source of numerical instability, notably when \(q_\phi(z|x) \ll p(z)\). In such circumstance, gradients during backpropagation can become extremely large (since gradients are inversely proportional to the denominator): e.g. for \(f(x)=\frac{1}{x}\), the gradient with respect to \(x\) is: \(\frac{\partial f}{\partial x}=−\frac{a}{x^2}​\).</p> </blockquote> <p>One solution is to transform \(p(x)\) into \(\log p(x)\). It won’t affect the finding of the right probability because the monotonicity of logarithm, and it moves denominator to the right of minus sign. Numerically speaking, additivity is better than multiplictivity. Thus, we have now</p> \[\log p(x)\simeq\mathbb{E}_{q_\phi(z|x)}[\log p(z) + \log p_\theta(x|z) - \log q_\phi(z|x)].\] <p>However, it should be noted that with the introduction of \(\theta\) and \(\phi\) as the modeling effort, the equation of \(p(x)\) doesn’t hold strictly. We should still depart from the strictly-holing chain rule of probability and see what’s the relationship between them two formulae:</p> \[\begin{aligned} \log p(x) &amp;=\log p(x)\int{q_\phi(z|x)dz} \quad \text{To introduce modeled sampler. Global Integral of probability is 1.}\\ &amp;=\int{\log p(x)q_\phi(z|x)dz} \quad \text{p(x) is function of x, resembling a constant for the integral about z.}\\ &amp;=\mathbb{E}_{q_\phi(z|x)}[\log p(x)] \quad \text{Definition of expectation.}\\ &amp;=\mathbb{E}_{q_\phi(z|x)}[\log \frac{p(z)p(x|z)}{p(z|x)}]\quad \text{Chain rule of probability.} \\ &amp;=\mathbb{E}_{q_\phi(z|x)}[\log p(z) + \log p(x|z) - \log p(z|x)]\quad \text{Split summation.} \end{aligned}\] <p>The sampler \(q_\phi(z|x)\) can be seen as a conditional probability density function over \(z\) given \(x\). So far it is still holding strict as the true \(\log p(x)\). Intuitively, we would like to replace \(p(z|x)\) with our modeled \(q(z|x)\), which obviously brings the ‘cost’ in so doing: the deviation. In this case, we need to have a <strong>measure of distance between two distributions</strong>, which should be a nonzero number and should be within the range of 0 to 1 for two normalized distributions to be compared.</p> <p>There comes a second <em>deus ex machina</em> invocation:</p> <blockquote> <p>KL divergence:</p> </blockquote> <blockquote> \[D_{KL}(P||Q)=\mathbb{E}_P[\log \frac{P(x)}{Q(x)}]=\mathbb{E}_P[\log P(x) -\log Q(x)],\] </blockquote> <p>which is originally used to measure the difference between two distributions \(P\) and \(Q\) in the formula across all values of the concerned variables. Its most important feature is that it is never goes negative, which provides quantitative relationship between the true and simulated \(\log p(x)\). This is ensured by logarithm’s concavity (i.e. Jensen’s inequality):</p> \[D_{KL}(P||Q)=\mathbb{E}_P[\log \frac{P}{Q}] \geq -\log \mathbb{E}_P[\frac{Q}{P}]=-\log \int P(x)\frac{Q(x)}{P(x)}dx =-\log\int Q(x)dx = -\log(1)=0\] <p>thanks to</p> \[\log⁡(\sum_i\lambda _ix_i)\geq\sum_i\lambda_i\log⁡(x_i).\] <p>For a convex combination \(\sum_i\lambda_i=1\) and \(\lambda_i\geq0\), and the equality of \(D_{KL}(P||Q)\) holds only if \(P=Q\). It can be understood by thinking of the secant line lies below the curve for logarithm function.</p> <p>In fact,</p> \[D_{KL}(P || Q)=H(P,Q)-H(P).\] <p>like the gap between \(H(P,Q)\) vs. \(H(P)\). And, given the unabandoned \(\log\), we come to notice that there is a asymmetry in \(D_{KL}\):</p> \[D_{KL}(P||Q) \neq D_{KL}(Q||P)\] <p>So, \(D_{KL}(P||Q)\) ignores regions where \(Q(x)&gt;P(x)\) if \(P(x)\approx 0\), due to the weight \(P\).</p> <p>Observe the simulated \(\log p(x)\) again, we find that the sampler under expectation notation can form a \(D_{KL}\) with \(p(z|x)\).</p> \[\begin{aligned} &amp; \mathbb{E}_{q_\phi(z|x)}[\log p(z) + \log p(x|z) - \log p(z|x)] \\ &amp;=\mathbb{E}_{q_\phi(z|x)}[\log p(z) \underbrace{- \log q_\phi(z|x) + \log q_\phi(z|x)}_{=0} + \log p(x|z) - \log p(z|x)] \\ &amp;=\mathbb{E}_{q_\phi(z|x)}[\log p(z) - \log q_\phi(z|x) + \log p(x|z) + (\log q_\phi(z|x) - \log p(z|x))] \quad\text{Switch summation order.}\\ &amp;=\mathbb{E}_{q_\phi(z|x)}[\log p(z) - \log q_\phi(z|x) + \log p(x|z) ] + \underbrace{\mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x) - \log p(z|x)]}_{i.e. D_{KL}(q_\phi(z|x) || p(x|z))\geq 0.} \\ &amp;\geq \mathbb{E}_{q_\phi(z|x)}[\log p(z) - \log q_\phi(z|x) + \log p(x|z) ] \equiv \mathbb{E}_{q_\phi(z|x)}[\log \frac{p(x,z)}{q_\phi(z|x)}] \end{aligned}\] <p>The last line of the equations is the expression of the so-called Evidence Lower Bound (ELBO). Most literature online prefers the form in tight fraction. The name Evidence comes again from the chain rule of probability:</p> \[\underbrace{p(x)}_{\text{evidence}} = \frac{ \underbrace{p(x, z)}_{\text{joint probability}} }{ \underbrace{p(z|x)}_{\text{posterior}} }.\] <p>We have developed the approaches of estimating the probability of the observed data and of comparing them with the true probability distribution in this framework all along. And</p> \[\log p(x) \geq \mathbb{E}_{q_\phi(z|x)}[\log p(z) - \log q_\phi(z|x) + \log p(x|z) ] \equiv \mathbb{E}_{q_\phi(z|x)}[\log \frac{p(x,z)}{q_\phi(z|x)}]\] <p>gives the name to Lower Bound.</p> <p>We now use ELBO to be the measure so that the corresponding value of observing those \(x\) in modeled distributions can be lower than that in the true distribution just because the modeled distribution is not close to the true distribution.</p> <p>The inequality about ELBO holds with a true \(p(x|z)\) being known, which is actually never the case in real life. A straightforward method to get the true distribution is to guess all the possible distribution to see which one provides the highest ELBO, but this is surely unfeasible. <strong>ELBO is now almost completely practicable</strong>, except for the numerator \(p(x,z)\) that can be further decomposed as \(p(z)p_\theta(x|z)\) where the \(_\theta\) indicates the modeling effort in decoder. However, it should be noted that the decomposition brings in approximation error via the modeling \(\theta\), so we have in effect:</p> \[\mathbb{E}_{q_\phi(z|x)}[\log \frac{p(x,z)}{q_\phi(z|x)}] \sim \mathbb{E}_{q_\phi(z|x)}[\log \frac{p(z)p_\theta(x|z)}{q_\phi(z|x)}]\] <p>I put \(\sim\) there instead of equal sign.</p><hr /> <p>In summary, people explore the hypothesizing structure of latents within the encoder-decoder methodology, hoping that by poking around the unknown universe of the mechanism of the true distribution of the observed data \(x\), some opportunities of improving tractability can be created in terms of modeling. That is what we will discuss in the next chapter. After all, we are relieved to know that our focus has by far transferred from the incomputable \(\log p(x)\) to the promising proxy objective ELBO now.</p> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
