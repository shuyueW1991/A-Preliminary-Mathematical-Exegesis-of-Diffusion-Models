{"0": {
    "doc": "Chapter 0 - Preface",
    "title": "Chapter 0 - Preface",
    "content": "Chapter 0 - Preface Generative AI has transformed countless aspects of our world. Among its many techniques, the diffusion model stands out as a groundbreaking framework—powering innovations like Stable Diffusion, whose magic captivates anyone (myself included) who remembers a time before the AI revolution. Yet, for all its dazzling applications, the inner workings of diffusion models often elude clear understanding. For learners like me, most tutorials and code snippets in blogs and papers provide only fragmented insights—forcing us to piece together scattered, incomplete explanations. Even seemingly thorough articles often rest on shallow foundations—skimming over core principles, whether from hurried exposition or an unspoken assumption that they’re ‘obvious.’ Sometimes, relentless onslaught of mathematical tricks can leave you nodding along—”Sure, I follow”—only to later wonder, “But what does this actually mean to do?” Over time, these unresolved questions pile up, until the aspiring learners simply walk away. I understand the frustration all too well. The fundamental idea behind diffusion models is rarely stated outright in most tutorials: like all generative AI, diffusion models attempt to aim to replicate what the nature offers—most often, images—in our own way. As for the term “diffusion”, it derives from a specific component of its underlying mechanism, which will be examined in detail in Chapter 3 of this booklet. Prior to delving into its technical foundations, we first establish the overarching objective shared by all generative AI paradigms: the development of models capable of synthesizing natural images. As the discussion progresses, the distinctive characteristics inherent to diffusion models will emerge. While preparing for a lecture on diffusion model and its SOTA applications at Emzan Technology Co. Ltd. for their department of autoslide.cc, I realized something: many of us needs a booklet on diffusion models that is mathematically rigorous yet accessible, one that strips away the noise and delivers the true, deep intuition behind the framework. This booklet is my attempt to share what I’ve learned about diffusion models—both the math behind them and how they’ve evolved via logic. I’ve tried to write it like something where ideas build naturally. The math is precise by my best, but I hope it feels more like following a story than reading a textbook. I’ve structured this booklet to guide readers naturally through diffusion models, like following a current. You won’t find rigid sections or numbered references because the focus isn’t on compartmentalized facts—it’s on how the ideas connect. Nothing’s introduced without context, and no idea exists just for show. I will slow down where I once struggled, so the readers might have an easier time. The booklet’s theoretical progression relies on a few carefully chosen deus ex machina elements—unavoidable but kept to a minimum. The term deus ex machina (Latin for “god from the machine”) originates from ancient Greek theater, where an external intervention abruptly resolved a tangled plot. In this context, it refers to key assumptions or mathematical tools that enable deductions which might otherwise seem unmotivated. Each such device will be explicitly introduced and justified: . | The use of Chebyshev inequality to bound the dot product between high-dimensional data. | The introduction of latent variables (denoted as z) as a modeling construct. | The use of KL divergence to measure distributional distances, along with its mathematical properties and role in derivations. | The Itô SDE and Fokker-Planck equation that evolves the sampled data distribution in the long run. | Central Limit Theorem, that provides a mathematically convincing path to the beautiful Gaussian distribution. | . By making these deliberate concessions explicit, the booklet ensures readers aren’t left puzzling over sudden leaps in reasoning. The booklet is divided into four chapters: . | Chapter 1 dismantles the seemingly simple mantra “maximize p(x)”—an idea so deceptively complex that it demands an entire chapter. | Chapter 2 offers a clear, concise derivation of the ELBO. | Chapter 3 unlocks the soul of diffusion model, i.e. the perspective of distribution transition that brings score into the game. | Chapter 4 implements the learnt mathematics into robust, actionable practices. | . For what it’s worth, don’t skim through this booklet. And remember that this is not a cookbook with code snippets, but rather the mathematical bridge between ideas and codes. As you explore this booklet, I hope you’ll feel the deep satisfaction of genuine intellectual engagement. I’ve written this with our community of practitioners in mind: learners who crave not just theoretical clarity, but practical mastery. Every page is designed to equip you with knowledge that’s both immediately useful in your work and foundational to advancing our collective grasp of this field. By the end, I hope you’ll grasp diffusion models so intuitively that you could explain them in your own words—not because you memorized the text, but because the ideas truly clicked. If I’ve done my job right, these concepts won’t feel like borrowed knowledge; they’ll feel like yours. Near Бишкек, Кыргызстан . June, 2025 . ",
    "url": "/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/00-chpt0.html",
    
    "relUrl": "/00-chpt0.html"
  },"1": {
    "doc": "Chapter 1 - The High-Dimensional Structure of True Data",
    "title": "Chapter 1 - The High-Dimensional Structure of True Data",
    "content": "Chapter 1 - The High-Dimensional Structure of True Data . We kickstart our exegesis by reiterating the fundamental objective of diffusion models claimed in the preface—To replicate what the nature offers, mostly in image generation. Consider a photograph taken from Shangrī-La, a hand-drawn portrait in the Musée du Louvre, or plants documented in Amazonía —each constitutes a manifestation of nature’s visual repertoire. We formally designate such representations as natural images. It is not difficult to understand, though, that being ‘natural’ for images is not the same with ‘real’. A “real” image is bound by the constraints of physical reality, whereas a “natural” image may include artistic interpretations, stylized elements, or even idealized forms that still feel intuitively coherent. For instance, an impressionist painting or a dreamlike digital rendering can qualify as natural despite departing from strict realism. Diffusion models, therefore, aim not just for photorealism but for the broader fidelity to natural visual grammar—the interplay of light, form, and texture that defines recognizability. This subtle distinction underscores their flexibility in generating both lifelike and creatively altered outputs while remaining anchored in naturalistic principles. In each of the above visual case, the visual input can be represented as a tensor—for instance, a \\(256 \\times 256\\) image admits \\({2^8}^3 = 16,777,216\\) possible colors per pixel (where 3 represents 3 RGB channels and 8 refers to the bit depth of the channel), and the total configuration space for pixel combinations therefore contains \\((16,777,216)^{256 \\times 256}\\) distinct images—a vast number that exceeds the estimated \\(10^{80}\\) atoms in the observable universe. If we randomly assign values to each pixel, the result will be more likely to appear chaotic than to be ‘natural’, almost certainly degrading into meaningless noise. This can be empirically verified by generating random arrays (e.g., via numpy.random.rand()in python) and visualizing them using any standard image display tool. Such a disordered array of uncorrelated hues would diverge sharply from the characteristics of a typical natural image. This is because natural images exhibit a structured harmony, where pixels organize into coherent objects, textures, and spatial relationships—manifesting as edges, discernible shapes, and smooth luminance gradients. While mathematically valid as tensors, these random permutations are exploiting the full combinatorial space without yielding meaningful representation. In fact, only a vanishingly small fraction of these possible configurations correspond to meaningful images. The sparsity of meaningful images might suggest Gaussian distributions as potential models, given their prevalence in other domains. However, Gaussian approximations fail dramatically for natural images. When fitted to image data through mean and covariance, sampling yields only blurry, unrealistic outputs if using Gaussian approximations. You can also try it by yourself. The mean can be an indistinct average (like an “average face”), while the covariance captures merely pairwise pixel relationships. This failure illustrates a deeper truth: semantics has finer structure than simple enumerations. These fundamental inadequacies necessitate more sophisticated approaches—such as diffusion models—capable of capturing the intricate structures inherent in natural images. Since arbitrary pixel arrangements rarely yield realistic outputs, we turn to probability theory for a solution, because real natural images follow certain patterns (e.g., smooth textures, recognizable objects), we can model distribution that registers those patterns statistically rather than relying on randomness. Here, each natural image can be viewed as a sampled event from an underlying probability distribution over all possible pixel configurations. In probability theory, an event represents a possible outcome of an experiment or observation. It is denoted by \\(x\\), which may be either a scalar in one dimension or a tensor in multiple dimensions. When considering images, \\(x\\) typically represents either meaningful natural images or random arrangements of pixels. The probability \\(p(x)\\) then quantifies the likelihood of event \\(x\\) occurring. It calculates the likelihood of observing a specific natural image among all possible pixel configurations. In this framework, every natural image thus represents a sample drawn from an underlying probability distribution governing all possible pixel configurations. A good \\(p(x)\\) should approach zero for random noise while assigning higher probability density to natural images. With this foundation, our focus will now shift entirely to modeling the true data distribution \\(p_{data}(x)\\), the underlying probability structure governing natural images. For practitioners seeking to leverage generative AI—whether to develop custom models or satisfy theoretical curiosity—understanding this distribution’s properties proves essential. Now let’s take a look at the high-dimensional tensor of noise with \\(d\\) pixels, each being independent with identically distributed (i.e. i.i.d.) components.: . \\[X=(X_1,X_2,…,X_d)\\] We suppose \\(X_i\\) has finite variance \\(\\sigma^2\\). Since: . \\[∥X∥_2^2=\\sum_{i=1}^dX_i^2\\] we have  . \\[\\mathbb{E}[\\|X\\|_2^2]=d\\cdot\\mathbb{E}[X_i^2]\\] and . \\[Var(\\|X\\|_2^2)=d⋅Var(X_i^2).\\] We calculate the ratio between the standard deviation and the mean, and we refer to this ratio as concentration ratio: . \\[\\frac{\\sqrt{Var(∥X∥_2^2)}}{E[∥X∥_2^2]}=\\frac{\\sqrt{d⋅Var(X_i^2)}}{d⋅E[X_i^2]} \\propto \\frac{1}{\\sqrt{d}}→0\\quad\\text{as }d \\rightarrow \\infty.\\] This vanishing result implies the concentration around the mean, which you can imagine a thin shell of noises shows itself gathering the sample when the dimension increases. We now illustrate this concentrated norm phenomenon for high-dimensional noise in uniform and gaussian distributions, respectively — . For \\(X_i \\sim \\text{Uniform}(-a,a)\\)，its probability density function is . \\[f_{X_i}(x) = \\frac{1}{2a}.\\] The variance is: . \\[\\begin{align*} \\text{Var}(X_i^2) &amp;= \\mathbb{E}[X_i^4] - (\\mathbb{E}[X_i^2])^2 \\\\ &amp;= \\left( \\int_{-a}^a x^4 \\cdot \\frac{1}{2a} \\, dx \\right) - \\left( \\int_{-a}^a x^2 \\cdot \\frac{1}{2a} \\, dx \\right)^2 \\\\ &amp;= \\left( \\frac{1}{2a} \\left[ \\frac{x^5}{5} \\right]_{-a}^a \\right) - \\left( \\frac{1}{2a} \\left[ \\frac{x^3}{3} \\right]_{-a}^a \\right)^2 \\\\ &amp;= \\frac{a^4}{5} - \\left( \\frac{a^2}{3} \\right)^2 \\\\ &amp;= \\frac{4a^4}{45} \\end{align*}\\] For \\(X = (X_1, \\dots, X_d)\\), we have: . \\[\\mathbb{E}[\\|X\\|_2^2] = d \\cdot \\mathbb{E}[X_i^2] = d \\cdot \\frac{a^2}{3},\\] so . \\[\\|X\\|_2 \\approx \\sqrt{\\mathbb{E}[\\|X\\|_2^2]} = a \\sqrt{\\frac{d}{3}}.\\] Also, we have: . \\[\\text{Var}(\\|X\\|_2^2) = d \\cdot \\text{Var}(X_i^2) = d \\cdot \\frac{4a^4}{45},\\] so . \\[\\sigma_{\\|X\\|_2^2} = \\frac{2a^2 \\sqrt{d}}{\\sqrt{45}} = \\frac{2a^2 \\sqrt{d}}{3 \\sqrt{5}}.\\] Then we now have the concentration ratio vanishing in the uniform distribution: . \\[\\frac{\\sigma_{\\|X\\|_2^2}}{\\mathbb{E}[\\|X\\|_2^2]} = \\frac{2a^2 \\sqrt{d} / (3 \\sqrt{5})}{d a^2 / 3} = \\frac{2}{\\sqrt{5d}}\\to 0\\text{ as }d \\to \\infty.\\] For \\(X_i \\sim \\mathcal{N}(0, \\sigma^2)\\), its probability density function is: . \\[f_{X_i}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{x^2}{2\\sigma^2}}.\\] The variance of \\(X_i^2\\): . \\[\\begin{align*} \\text{Var}(X_i^2) &amp;= \\mathbb{E}[X_i^4] - \\left(\\mathbb{E}[X_i^2]\\right)^2 \\\\ &amp;= \\left( \\int_{-\\infty}^{\\infty} x^4 f_{X_i}(x) \\, dx \\right) - \\left( \\int_{-\\infty}^{\\infty} x^2 f_{X_i}(x) \\, dx \\right)^2 \\\\ &amp;= \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty} x^4 e^{-\\frac{x^2}{2\\sigma^2}} \\, dx \\right) - \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\int_{-\\infty}^{\\infty} x^2 e^{-\\frac{x^2}{2\\sigma^2}} \\, dx \\right)^2 \\\\ &amp;= \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot 3\\sqrt{2\\pi}\\sigma^5 \\right) - \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\sqrt{2\\pi}\\sigma^3 \\right)^2 \\\\ &amp;= 3\\sigma^4 - (\\sigma^2)^2 \\\\ &amp;= 2\\sigma^4 \\end{align*}\\] For \\(X = (X_1, \\dots, X_d)\\), we have: . \\[\\mathbb{E}[\\|X\\|_2^2] = d \\cdot \\mathbb{E}[X_i^2] = d \\sigma^2,\\] so . \\[\\|X\\|_2 \\approx \\sqrt{\\mathbb{E}[\\|X\\|_2^2]} = \\sigma \\sqrt{d}.\\] And we have . \\[\\text{Var}(\\|X\\|_2^2) = d \\cdot \\text{Var}(X_i^2) = 2d \\sigma^4,\\] so . \\[\\sigma_{\\|X\\|_2^2} = \\sigma^2 \\sqrt{2d}.\\] Then the concentration ratio in the gaussian distribution becomes: . \\[\\frac{\\sigma_{\\|X\\|_2^2}}{\\mathbb{E}[\\|X\\|_2^2]} = \\frac{\\sigma^2 \\sqrt{2d}}{d \\sigma^2} = \\sqrt{\\frac{2}{d}} \\to 0 \\text{ as } d \\to \\infty.\\] Now let’s check upon the relationship between the noises inside the shell. Consider two random noises \\(\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^d\\) , with i.i.d. entries , each has finite variances and bounded fourth moments: . \\[\\text{Var}(u_i) = \\sigma_u^2, \\text{Var}(v_i) = \\sigma_v^2 &lt; \\infty,\\text{and } \\mathbb{E}[u_i^4], \\mathbb{E}[v_i^4] &lt; \\infty\\] The dot product is . \\[\\langle \\mathbf{u}, \\mathbf{v} \\rangle \\triangleq \\sum_{i=1}^d u_i v_i\\] has expectation: . \\[\\mathbb{E}[\\langle \\mathbf{u}, \\mathbf{v} \\rangle] = \\sum^d_{i=1}\\mathbb{E}[u_iv_i] = \\sum^d_{i=1}\\mathbb{E}[u_i]\\mathbb{E}[v_i] = d \\mu_u \\mu_v.\\] The split of expectation comes from i.i.d. features. Its variance is: . \\[\\text{Var}(u_i v_i) = \\mathbb{E}[u_i^2 v_i^2] - (\\mathbb{E}[u_i v_i])^2 = (\\sigma_u^2 + \\mu_u^2)(\\sigma_v^2 + \\mu_v^2) - \\mu_u^2 \\mu_v^2=\\sigma^2​_u\\sigma_v^2​+\\sigma^2​_v\\mu_u^2​+\\sigma^2​_u\\mu_v^2​.\\] Here comes the first deus ex machina of the booklet: . The Chebyshev inequality provides a bound on the probability that a random variable \\(X\\) deviates from its mean \\(\\mu\\) by more than \\(k\\) standard deviations \\(\\sigma\\), stating that \\(\\Pr(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}\\) for any \\(k &gt; 0\\). This universal bound applies to any distribution with finite variance, offering a measure of dispersion guarantees. Back to our case, for \\(\\epsilon &gt; 0\\): . \\[\\mathbb{P}\\left( \\left| \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{d} - \\mu_u \\mu_v \\right| \\geq \\epsilon \\right) \\leq \\frac{\\text{Var}(\\langle \\mathbf{u}, \\mathbf{v} \\rangle)}{d^2 \\epsilon^2} = \\frac{d\\cdot (\\sigma^2​_u\\sigma_v^2​+\\sigma^2​_v\\mu_u^2​+\\sigma^2​_u\\mu_v^2)}{d^2 \\epsilon^2} \\to 0 \\text{ as } d \\to \\infty.\\] Thus, equivalently: . \\[\\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{d} \\to \\mu_u \\mu_v.\\] The squared norm of \\(\\mathbf{u}\\) is: . \\[\\|\\mathbf{u}\\|^2 = \\sum_{i=1}^d u_i^2.\\] So, compute the expected value: . \\[\\mathbb{E}\\left[\\frac{\\|\\mathbf{u}\\|^2}{d}\\right] = \\frac{1}{d} \\sum_{i=1}^d \\mathbb{E}[u_i^2].\\] Since . \\[\\mathbb{E}[u_i^2] = \\text{Var}(u_i) + (\\mathbb{E}[u_i])^2 = \\sigma_u^2 + \\mu_u^2,\\] we have: . \\[\\mathbb{E}\\left[\\frac{\\|\\mathbf{u}\\|^2}{d}\\right] =\\frac{d\\cdot(\\sigma_u^2 + \\mu_u^2)}{d}= \\sigma_u^2 + \\mu_u^2.\\] Then, similar to the use of the Chebyshev inequality, the squared norms concentrate as: . \\[\\frac{\\|\\mathbf{u}\\|^2}{d} \\to \\sigma_u^2 + \\mu_u^2, \\quad \\frac{\\|\\mathbf{v}\\|^2}{d} \\to \\sigma_v^2 + \\mu_v^2.\\] The normalized dot product thus satisfies: . \\[\\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|} = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle / d}{\\sqrt{(\\|\\mathbf{u}\\|^2 / d)(\\|\\mathbf{v}\\|^2 / d)}} \\to \\frac{\\mu_u \\mu_v}{\\sqrt{(\\sigma_u^2 + \\mu_u^2)(\\sigma_v^2 + \\mu_v^2)}}.\\] From the definniton of inner vectorial product, if \\(\\mu_u \\mu_v = 0\\) (i.e. at least one mean is zero), there is orthogonality between noises within the shell. That is to say, for zero-mean random vectors in high dimensions, if the distribution to sample is zero-meaned, the sampled vectors are likely to be orthogonal to each other. Under this circumstance, the directions of noise vectors remain uniformly distributed over the sphere, which is quite unusual in low-dimensional space. For a more intuitive grasp of high-dimensional noise , let’s check how a perturbation added to a high-dimensional tensor would be like. Consider a random noise vector \\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^d\\), where each component \\(\\epsilon_i\\) is sampled i.i.d. from a distribution with mean \\(0\\) and variance \\(\\sigma^2\\). By Chebyshev’s inequality, the normalized squared norm \\(|\\boldsymbol{\\epsilon}|_2^2/d\\) concentrates around \\(\\sigma^2\\): . \\[\\Pr\\left(\\left|\\frac{\\|\\boldsymbol{\\epsilon}\\|_2^2}{d} - \\sigma^2\\right| \\geq \\delta\\right) \\leq \\frac{\\text{Var}(\\epsilon_i^2)}{d\\delta^2},\\] implying \\(|\\boldsymbol{\\epsilon}|_2 \\approx \\sigma\\sqrt{d}\\) for large \\(d\\). It means that even small noise (\\(\\sigma\\)) becomes significant in high dimensions, causing large prediction changes. For example, a linear model \\(f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x}\\), the perturbation’s impact scales as \\(|\\mathbf{w}^T \\boldsymbol{\\epsilon}| \\leq |\\mathbf{w}|_2 \\sigma \\sqrt{d}\\). In the context of images, small perturbations to pixel values can induce large changes in model predictions. For a given portrait, it takes not much of effort to make it not portrait-like any more by perturbing the pixels. The quasi-orthogonality of high-dimensional noise vectors and the concentration of their norms play crucial roles in diffusion models. These properties ensure efficient exploration of the data space during the forward process and stable reconstruction during denoising. Like a well-calibrated search algorithm, they enable the model to systematically perturb and smear entirely the high-dimensional space. We will analyze these in detail in later chapters. Our analysis of high-dimensional data naturally leads to the question: what distinguishes natural images within this space that is mostly occupied by noise? We take a look at their norms, first. Consider a \\(32 \\times 32\\) grayscale image, representing a vector in \\(\\mathbb{R}^{1024}\\) space (\\(d = 32 \\times 32 = 1024\\)). Let \\(\\mathbf{x} \\in \\mathbb{R}^d\\) denote the image and \\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^d\\) represent standard normal noise (\\(\\epsilon_i \\sim \\mathcal{N}(0, 1)\\)). The expected squared \\(L_2\\) norm of the noise is: . \\[\\mathbb{E}[\\|\\boldsymbol{\\epsilon}\\|_2^2] = \\sum^d_{i}\\mathbb{E}[\\|\\epsilon_i\\|_2^2] = d \\cdot \\sigma^2 = 1024 \\cdot 1 = 1024\\] which yields a typical noise magnitude of \\(|\\boldsymbol{\\epsilon}|_2 \\approx \\sqrt{1024} = 32\\). In contrast, studies have found that the average squared \\(L2\\)​ norm of CIFAR-10 images (after normalization) is roughly 200, yielding a typical norm of \\(\\sqrt{200}\\sim14.14\\), significantly smaller than that of a random noise. We find that natural images exhibit smaller norms compared to random noise with independent and identically distributed (i.i.d.) entries. Three key factors explain this phenomenon mathematically. First, natural images exhibit piecewise smoothness: they consist of regions with slowly varying pixel values (like smooth surfaces or uniform areas) separated by abrupt transitions (edges). Since edges occupy relatively few pixels, their contribution to the overall norm is minimal compared to smooth regions. Second, the presence of sharp edges creates heavy-tailed distributions. Unlike Gaussian distributions, natural images show higher probability mass in extreme values due to frequent abrupt transitions between regions. Third, the abundance of near-zero pixel values (from dark areas or low-intensity regions) further reduces the overall norm. Approximately 50% of pixels in natural images often fall into this category. This norm discrepancy demonstrates that natural images cannot be modeled as random high-dimensional vectors with i.i.d. components. Correlations indicated by smooth edges, textures, and objects imply that the data is not filling the high-dimensional space uniformly but instead concentrates near a lower-dimensional structure. Another perspective to understand this is through entropy. Entropy measures the average uncertainty (or information content) of event \\(X\\), where higher entropy means more randomness that makes compression harder, and lower entropy means more predictability that makes allowing better compression. Regarding flipping a coin, a fair coin (50% heads, 50% tails) should have maximum entropy because you’re completely uncertain about the outcome, while a biased coin (e.g., 99% heads) should have lower entropy, because you’re more confident it’ll land heads. A higher entropy should bring more surprise. In this spirit, Shannon’s entropy is thus defined: for a discrete random variable \\(X\\) with possible outcomes \\(x_i\\) and probabilities \\(p_i\\)​: . \\[H(X)=− \\sum_{i} ​p_i(X)​ \\log p_i(X)\\] For an image \\(X\\) with pixel  \\(x_i\\in{0,1,…,255}\\) as channel value, the entropy per pixel is: . \\[H_{noise}=-\\sum_{i=0}^{255}p(x_i)\\log_2p(x_i)\\] We consider random noise as uniformly distributed, then its entropy is . \\[-\\sum_{i=0}^{255}\\frac{1}{256}\\log_2\\frac{1}{256}=\\log⁡_2 256=8 \\text{bits/pixel}.\\] Knowing this, we now look at the joint entropy \\(H(X,Y)\\) that measures the total uncertainty of the pair \\((X,Y)\\): . \\[H(X,Y) = -\\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} P(x,y) \\log_2 P(x,y),\\] where \\(P(x,y)\\) is the joint probability of \\(X = x\\) and \\(Y = y\\). When \\(X\\) and \\(Y\\) are independent, the joint probability factorizes: . \\[P(x,y) = P(x)P(y).\\] Substituting into the joint entropy formula: . \\[H(X,Y) = -\\sum_{x} \\sum_{y} P(x)P(y) \\log_2 \\left[ P(x)P(y) \\right]= -\\sum_{x} \\sum_{y} P(x)P(y) \\left[ \\log_2 P(x) + \\log_2 P(y) \\right].\\] Splitting the sum into: . \\[-\\sum_{x} P(x) \\log_2 P(x) \\sum_{y} P(y) = H(X)\\] and . \\[-\\sum_{y} P(y) \\log_2 P(y) \\sum_{x} P(x) = H(Y).\\] Thus: . \\[H(X,Y) = H(X) + H(Y).\\] So, for an \\(n\\times n\\) image with \\(n^2\\) pixels, if pixel values are independent, then the total entropy is additive: . \\[H_{total}=n^2\\times H=n^2\\times 8 \\text{bits}.\\] Natural images have much lower entropy than random noise images, meaning they occupy an exponentially smaller fraction of the space of all possible pixel combinations. This is also concluded from empirical studies. Due to correlations, natural images have much lower entropy. An image of a face is determined by lighting, pose, and identity, not pixel-by-pixel independently. Therefore, due to strong statistical dependencies, their effective entropy is significantly lower than their nominal dimensionality would suggest. This reduction in entropy implies that natural images possess far fewer degrees of freedom than independent pixel sampling would permit. Natural images do not fill their high-dimensional pixel space uniformly; instead, they reside in an exponentially sparse subset. On one hand, natural images are highly structured, more complex than a random noise. And they have smaller norms compared with a random noise. On the other hand, we have known that natural images are vanishingly rare in the space of all possible pixel combinations. Randomly sampling from all possible \\(256\\times256\\) RGB images gives you almost 0 probability of getting a natural image.   But if you sample from real-world camera outputs, the 0 probability goes to the meaningless noise, It does no harm to imagine that maybe, natural images dominate the probability mass of the true data distribution \\(p_{data}(x)\\) i.e. it is highly peaked around natural images. Thus, while natural images may be rare in the global view of high-dimensional space, they dominate within certain localized regions. This structure is formally referred to as a manifold (\\(\\mathcal{M}\\)) in the machine learning community—a low-dimensional geometric object embedded within a high-dimensional space. A useful analogy is a two-dimensional sheet of paper floating in three-dimensional space. Globally, the paper occupies no volume—it is infinitely thin—yet every point on its surface lies precisely within its own two-dimensional plane. Similarly, natural images, though sparse in the full pixel space, concentrate on much lower-dimensional manifolds where their intrinsic structure resides. Mathematically speaking, in the space of all possible images, the set of natural images has extremely small volume but very high density \\(p_{data}(x)\\) for \\(x\\in M\\). The rest of the space (noise, random pixels) has enormous volume but near-zero density \\(p_{data}(x)\\approx 0\\). Sampling from \\(p_{data}\\)​ thus almost always gives natural images. When generating new images, there might be chances of wandering into low-probability regions. However, if your model \\(p(x)\\) is good enough, sampling from it correctly will almost always give natural images, because in a well-trained p(x), almost all the probability mass is concentrated around the natural image manifold. Note that probability mass ≠ size of set: this is the key insight! The size of a set (e.g., natural images vs. noise) is not the same as its probability mass. Probability mass depends on the distribution \\(p(x)\\). If you randomly sample pixels (uniformly), you’ll almost always get noisy garbage. but if you cast a regard to real world, you rarely see random noise. Therefore, a well-learned generative model should resemble this true data distribution where the high-density regions should correspond to natural images, and the low-density regions should correspond to noise or unrealistic images. They shrink the sampling space to near \\(\\mathcal{M}\\), making natural images likely outputs. If we would like to realize such a modeled distribution, we can use a suitable model family chosen as \\(p_\\theta(x)\\), with unknown parameters \\(\\theta\\), so that the problem boils down to maximizing the average (log-)likelihood (w.r.t \\(\\theta\\)) of all the samples under the model . \\[\\theta^* = \\arg \\max_\\theta \\mathbb{E}_{x \\sim q_{\\text{data}}(x)} [\\log p_\\theta(x)] \\approx \\arg \\max_\\theta \\frac{1}{N} \\sum_{n=1}^N \\log p_\\theta(x^{(n)})\\] where defining an arbitrary parametric density \\(p_\\theta(x)\\) is not as easy as it looks. In summary, in the space of all pixel combination, even though natural images are a tiny subset of all possible images, they carry almost all the probability mass in \\(p(x)\\). This is the benefit of getting a modeled distribution that is close to true distribution. Equivalently, the closer a modeled distribution to the true distribution, the more probability of observed data getting sampled. The problem of generative modeling can be posed as parametric density estimation using a finite set of observed samples \\({x^{(n)}}^N_{n=1}\\) from a true but unknown data distribution \\(p_{true}(x)\\). The observed data \\(x\\) are observed because there is a true distribution out there, and the data point represented by the observed data occupies most of the probability mass of the true distribution. There should be a value measuring the probability of observing those \\(x\\). ",
    "url": "/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/01-chpt1.html",
    
    "relUrl": "/01-chpt1.html"
  },"2": {
    "doc": "Chapter 2 - The ELBO Paradigm --- Proxy Objective for True Data Maximization",
    "title": "Chapter 2 - The ELBO Paradigm --- Proxy Objective for True Data Maximization",
    "content": "Chapter 2 - The ELBO Paradigm --- Proxy Objective for True Data Maximization . In previous chapter, we have talked about finding the best modeled distribution via evaluating the likelihood \\(p(x)\\)  of observed or generated data, because that measures how well they explain the observation: higher likelihood suggests a better model. Question now is: how to maximize the \\(p(x)\\), i.e. optimize \\(p(x)\\) to achieve maximal likelihood? . In this chapter, to get this game going, we will introduce two deus ex machina magics. The first one in this chapter is . the latent variables, denoted as \\(z\\). The incorporation of latent variables in generative models represents a powerful paradigm grounded in critical insights about data structure. Real-world datasets often exhibit complex dependencies of underlying, unobserved factors. For instance, image data contains implicit attributes like illumination geometry and object orientation that are not explicitly encoded in pixel values but significantly influence the observed patterns. By explicitly representing these hidden factors, latent variables enable models to capture richer data structure. Without latent variables, we would face the formidable challenge of modeling complex high-dimensional distributions directly - such as capturing intricate pixel-level correlations in images - which is both computationally intractable and statistically inefficient. Latent variables decompose the problem into more manageable components: a simple latent space distribution \\(p(z)\\) and a conditional data distribution \\(p(x|z)\\), where \\(z\\) represents the latent factors and \\(x\\) the observed data. This framework offers a structured, interpretable, and scalable approach—transforming an intractable problem into one where hidden factors systematically explain observed phenomena. In fact, the metaphor of Plato’s Cave—from The Republic—provides a powerful analogy for understanding latent variables in generative models. In Plato’s allegory, prisoners are chained in a cave, seeing only shadows cast on a wall by objects they cannot directly observe. This mirrors the relationship between observed data and latent variables. The shadows are like our raw data (e.g., pixel values in images), mere surface-level projections. The true forms are the latent variables—the unobserved, higher-dimensional factors (e.g., lighting, pose, or semantic meaning) that generate the data, just as the objects outside the cave cast the shadows. In both cases, reality is richer than what we directly perceive. Latent variables act as the hidden causes behind the observable effects, allowing models to infer the underlying structure that shapes the data. We now try maximizing \\(p(x)\\) by utilizing latents \\(z\\). We’re here to find the distribution in which the observed data \\(x_1, x_2, …, x_n\\) would be highest in probability compared with other distributions. In modeling terms, this translates to finding a configuration of model parameters  such that the observed data have the highest probability compared with other configuration. Keep in mind that it has never been a problem of probability value arithmetic, as is suggested by its denotation format; it is a distribution (i.e. the best \\(p\\), in whichever form it takes, that we are actually looking for). One way of linking latents \\(z\\) to \\(p(x)\\) is to think about: . \\[p(x)=\\int{p_\\theta(x,z)dz}=\\int{p_\\theta(x|z)p(z)dz}\\] where \\(p_\\theta(x,z)\\) refers to a new modeled (indicated by \\(_\\theta\\)) probability distribution (indicated by \\(p\\)) with new set of params (indicated by \\(x,z\\) ). The decomposition of integrand is directly from the chain of rule in probability. \\(p(x|z)\\), a.k.a. decoder, describes how observations \\(x\\) are generated from latents \\(z\\); and \\(p(z|x)\\), a.k.a. encoder, describes how latents can be inferred from observations. Note that the equation holds regardless of whether \\(x\\) and \\(z\\) are independent: independence would imply \\(p(x,z)=p(x)p(z)\\), but this is not necessary for the marginalization to be valid. With this, one might propose the procedures of estimate \\(p(x\\)): . | sample  \\(z\\) from \\(p(z)\\) | for a given \\(\\theta\\), calculate \\(p_\\theta(x|z)\\) , i.e. the probability of observing the data \\(x\\) we have observed given a sampled \\(z\\). | do this with sufficient times of \\(z\\) sampling, thus get to compute the integral. | Therefore, by adjusting the model param \\(\\theta\\), the integral gets bigger or smaller correspondingly, until the \\(\\theta\\) corresponds to one high value of \\(p(x)\\) is found. | . However, the intractability of the integration a major challenge itself. \\(z\\) is typically a high-dimensional vector. The integral is over all possible values of \\(z\\), which is computationally infeasible for even moderate size. Example: if \\(z\\) has 100 dimensions and we discretize each dimension into just 10 values, the number of terms in the sum grows as much as \\(10^{100}\\). The decoder \\(p_\\theta(x|z)\\) , like other mathematical things in machine learning that is beyond explicit expression, is usually modeled by a neural network. To such a complex nonlinear function, there’s no analytical formula for integrating it over \\(z\\), even if \\(p(z)\\) is as simple as Gaussian. Suppose we repeat \\(N\\) times to yield the approximation: . \\[p_{\\boldsymbol{\\theta}}(x) = \\int p_{\\theta}(x|z)p(z)dz \\approx \\frac{1}{N}\\sum_{i=1}^N p_{\\theta}(x|z^{(i)})\\] This approximation converges to the true expectation as \\(N \\to \\infty\\) by the law of large numbers. We optimize parameters \\(\\theta\\) to maximize \\(p_{\\theta}(x)\\), thereby improving the model’s fit to the observed data while maintaining the learned low-dimensional structure. However, it takes very large number to await the valid \\(N\\) to come. In addition, most \\(z\\) samples obtained in this approach will contribute negligibly to \\(p_\\theta(x)\\), since \\(p(z)\\) is uninformed about \\(x\\), so most \\(z\\) values will lead to \\(p_\\theta(x∣z)≈0\\), just consider how much naturalimages occupies the whole space. Blindly sampling  \\(z \\sim p(z)\\) provides little guidance to the decoder about which regions of \\(z\\)-space are relevant for generating meaningful \\(x\\). A second approach of linking latents \\(z\\) to \\(p(x)\\) is through chain rule of probability: . \\[p(x)=\\frac{p(x,z)}{p(z|x)}\\] Since the latents \\(z\\) are in both numerator and denominators, no direct observations can be made. Given our goal is to maximize \\(p(x)\\), we’re here faced with the problem of maximizing two unknown functions distributions simultaneously. The denominator can be modeled as encoder network, and it can be denoted as \\(p_\\phi(z|x)\\). It is quite instinctive to imagine a mapping from observed data \\(x\\) towards latents \\(z\\). But in many literature, the \\(p_\\phi(z|x)\\) here is written as \\(q_\\phi(z|x)\\), a notation that emphasizes it’s an approximation of the true posterior \\(p(z|x)\\). Modeling the numerator  \\(p_\\theta(x,z)\\) might seem conceptually straightforward: we could simply introduce another parameterized distribution. But in practice, designing a network that simultaneously takes both observed data xx and latent variables \\(z\\) as inputs presents significant architectural challenges, making this approach less intuitive to implement. We consider use chain rule of probability again to decompose \\(p_\\theta(x,z)\\) into \\(p(z)p_\\theta(x|z)\\). Analogously, the \\(p_\\theta(x|z)\\) can be modeled as a decoder network. Now there’s the term of \\(p(z)\\) left. For the time being, we want the distribution to be of as much entropy as possible with fixed mean and variance. We’re now proving that with the definition of Shannon’s entropy, if there is constraints of given fixed mean . \\[\\mu = E[X]\\] and fixed variance . \\[\\sigma^2 = E[(X-\\mu)^2],\\] then it’s Gaussian distribution that maximizes \\(H(X)\\): . \\[p^*(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] The problem is essentially another constraint optimization problem to maximize differential entropy: . \\[h(X) = -\\int_{-\\infty}^{\\infty} p(x) \\log p(x) \\, dx\\] subject to: . \\[\\int_{-\\infty}^{\\infty}p(x)dx = 1 \\quad\\text{(Normalization)},\\] \\[\\int_{-\\infty}^{\\infty} x p(x)dx = \\mu \\quad \\text{(Mean)},\\] and . \\[\\int_{-\\infty}^{\\infty} (x-\\mu)^2 p(x)dx = \\sigma^2 \\quad \\text{(Variance)}\\] Now the Lagrangian functional is: . \\[\\begin{align*} \\mathcal{L}[p] = &amp;- \\int p(x) \\log p(x) \\, dx \\\\ &amp;+ \\lambda_1 \\left( \\int p(x) \\, dx - 1 \\right) \\\\ &amp;+ \\lambda_2 \\left( \\int x p(x) \\, dx - \\mu \\right) \\\\ &amp;+ \\lambda_3 \\left( \\int (x-\\mu)^2 p(x) \\, dx - \\sigma^2 \\right) \\end{align*}\\] As for a general functional of the form . \\[F[p] = \\int f(x, p(x), p'(x)) \\, dx，\\] the functional derivative is given by: . \\[\\frac{\\delta F}{\\delta p(y)} = \\frac{\\partial f}{\\partial p}\\bigg|_{x=y} - \\frac{d}{dx}\\left(\\frac{\\partial f}{\\partial p'}\\right)\\bigg|_{x=y}.\\] Term by term, we calculate: . \\[\\frac{\\delta}{\\delta p}\\left(-\\int p(x)\\log p(x) \\, dx\\right) = -\\left(\\log p(x) + p(x)\\cdot\\frac{1}{p(x)}\\right) = -\\log p(x) - 1,\\] \\[\\frac{\\delta}{\\delta p}\\left(\\lambda_1 \\int p(x) \\, dx\\right) = \\lambda_1,\\] \\[\\frac{\\delta}{\\delta p}\\left(\\lambda_2 \\int x p(x) \\, dx\\right) = \\lambda_2 x ,\\] and . \\[\\frac{\\delta}{\\delta p}\\left(\\lambda_3 \\int (x-\\mu)^2 p(x) \\, dx\\right) = \\lambda_3 (x-\\mu)^2.\\] Combining all terms, and setting it to zero: . \\[\\begin{align*} \\frac{\\delta\\mathcal{L}}{\\delta p} = -\\log p(x) - 1 + \\lambda_1 + \\lambda_2 x + \\lambda_3 (x-\\mu)^2 &amp;= 0 \\\\ \\implies \\log p(x) &amp;= -1 + \\lambda_1 + \\lambda_2 x + \\lambda_3 (x-\\mu)^2 \\\\ p(x) &amp;= \\exp\\left(-1 + \\lambda_1 + \\lambda_2 x + \\lambda_3 (x-\\mu)^2\\right) \\end{align*}\\] The exponential form can now be rewritten as: . \\[p(x) = e^{\\lambda_1 - 1} \\cdot e^{\\lambda_2 x} \\cdot e^{\\lambda_3 (x-\\mu)^2}\\] After completing the square and enforcing the constraints, we find: . \\[\\begin{align*} \\lambda_3 &amp;= -\\frac{1}{2\\sigma^2} \\\\ \\lambda_2 &amp;= \\frac{\\mu}{\\sigma^2} \\\\ e^{\\lambda_1 - 1} &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{\\mu^2}{2\\sigma^2}} \\end{align*}\\] Substituting these back gives the Gaussian distribution: . \\[p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] Among all distributions with a given mean and variance, the Gaussian has the highest entropy (i.e., it makes the fewest assumptions). This makes it a natural default choice when no additional structure is assumed. That dictates our prior to be . \\[p(z) \\sim \\mathcal{N}(\\mu, \\sigma^2).\\] To wrap up our second approach, the new way to estimate \\(p(x)\\) can be like this: . | sample one \\(z_i\\) (\\(i\\) being arbitrary integer as sampling index) from the variational posterior \\(q_\\phi(z|x)\\) by inputting one observed data \\(x_i\\) into the network. | one data point in true data dataset can be sampled more than one time. | with the \\(z_i\\), evaluate \\(p(x)\\) as \\(\\frac{p(z_i)p_\\theta(x|z_i)}{q_\\phi(z_i|x)}\\). | . An expectation (even approximated with a few samples) gives a smoother, more stable gradient, because averaging over multiple samples would reduce variance. So let there be multiple samples instead of only one, and average the sum of their values in the final \\(p(x)\\)’s, which is basically the idea of Monte Carlo estimating. Another concern is from \\(q_\\phi(z|x)\\): like all denominators, it is a headache for numerical computation. It may lead to high variance if that is a poor approximation of the true encoder, which is a source of numerical instability, notably when \\(q_\\phi(z|x) \\ll p(z)\\). In such circumstance, gradients during backpropagation can become extremely large (since gradients are inversely proportional to the denominator): e.g. for \\(f(x)=\\frac{1}{x}\\), the gradient with respect to \\(x\\) is: \\(\\frac{\\partial f}{\\partial x}=−\\frac{a}{x^2}​\\). One solution to solve the problem is to transform \\(p(x)\\) into \\(\\log p(x)\\). It won’t affect the finding of the right probability because the monotonicity of logarithm, and it moves denominator to the right of minus sign. Numerically speaking, additivity is better than multiplictivity. Thus, we have now . \\[\\log p(x)\\simeq\\mathbb{E}_{q_\\phi(z|x)}[\\log p(z) + \\log p_\\theta(x|z) - \\log q_\\phi(z|x)].\\] However, it should be noted that with the introduction of \\(\\theta\\) and \\(\\phi\\) as the modeling effort, the equation of \\(p(x)\\) doesn’t hold strictly. We should still depart from the strictly-holing chain rule of probability and see what’s the relationship between them two formulae: . \\[\\begin{aligned} \\log p(x) &amp;=\\log p(x)\\int{q_\\phi(z|x)dz} \\quad \\text{Introduce modeled sampler. Global Integral of probability is 1.}\\\\ &amp;=\\int{\\log p(x)q_\\phi(z|x)dz} \\quad p(x)\\text{functions on x, resembling a constant for the integral about z.}\\\\ &amp;=\\mathbb{E}_{q_\\phi(z|x)}[\\log p(x)] \\quad \\text{Definition of expectation.}\\\\ &amp;=\\mathbb{E}_{q_\\phi(z|x)}[\\log \\frac{p(z)p(x|z)}{p(z|x)}]\\quad \\text{Chain rule of probability.} \\\\ &amp;=\\mathbb{E}_{q_\\phi(z|x)}[\\log p(z) + \\log p(x|z) - \\log p(z|x)]\\quad \\text{Split summation.} \\end{aligned}\\] You can see each one of deduction is simple up there. The sampler \\(q_\\phi(z|x)\\) can be seen as a conditional probability density function over \\(z\\) given \\(x\\). So far it is still holding strict as the true \\(\\log p(x)\\). Intuitively, we would like to replace \\(p(z|x)\\) with our modeled \\(q(z|x)\\), which obviously brings the ‘cost’ in so doing: the deviation. In this case, we need to have a measure of calculating the distance between two distributions. The measuring result should be a nonzero number, and should be within the range of 0 to 1 for two normalized distributions to be compared. There comes a second deus ex machina invocation: . KL divergence, taking form of \\(D_{KL}(P||Q)=\\mathbb{E}_P[\\log \\frac{P(x)}{Q(x)}]=\\mathbb{E}_P[\\log P(x) -\\log Q(x)].\\) . which is originally used to measure the difference between two distributions \\(P\\) and \\(Q\\) in the formula) across all values of the concerned variables. Its most important feature is it is never goes negative, which provides quantitative relationship between the true and simulated \\(\\log p(x)\\). This is ensured by logarithm’s concavity (i.e. Jensen’s inequality): . \\[D_{KL}(P||Q)=\\mathbb{E}_P[\\log \\frac{P}{Q}] \\geq -\\log \\mathbb{E}_P[\\frac{Q}{P}]=-\\log \\int P(x)\\frac{Q(x)}{P(x)}dx =-\\log\\int Q(x)dx = -\\log(1)=0\\] thanks to . \\[\\log⁡(\\sum_i\\lambda _ix_i)\\geq\\sum_i\\lambda_i\\log⁡(x_i).\\] For a convex combination \\(\\sum_i\\lambda_i=1\\) and \\(\\lambda_i\\geq0\\), and the equality of \\(D_{KL}(P||Q)\\) holds only if \\(P=Q\\). It can be understood by thinking of the secant line lies below the curve for logarithm function. In fact, . \\[D_{KL}(P || Q)=H(P,Q)-H(P).\\] like the gap between \\(H(P,Q)\\) vs. \\(H(P)\\). And, given the unabandoned \\(\\log\\), we come to notice that there is a asymmetry in \\(D_{KL}\\): . \\[D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\] So, \\(D_{KL}(P||Q)\\) ignores regions where \\(Q(x)&gt;P(x)\\) if \\(P(x)\\approx 0\\), due to the weight \\(P\\). Observe the simulated \\(\\log p(x)\\) again, we find that the sampler under expectation notation can form a \\(D_{KL}\\) with \\(p(z|x)\\). \\[\\begin{aligned} &amp; \\mathbb{E}_{q_\\phi(z|x)}[\\log p(z) + \\log p(x|z) - \\log p(z|x)] \\\\ &amp;=\\mathbb{E}_{q_\\phi(z|x)}[\\log p(z) \\underbrace{- \\log q_\\phi(z|x) + \\log q_\\phi(z|x)}_{=0} + \\log p(x|z) - \\log p(z|x)] \\\\ &amp;=\\mathbb{E}_{q_\\phi(z|x)}[\\log p(z) - \\log q_\\phi(z|x) + \\log p(x|z) + (\\log q_\\phi(z|x) - \\log p(z|x))] \\quad\\text{Switch summation order.}\\\\ &amp;=\\mathbb{E}_{q_\\phi(z|x)}[\\log p(z) - \\log q_\\phi(z|x) + \\log p(x|z) ] + \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}[\\log q_\\phi(z|x) - \\log p(z|x)]}_{i.e. D_{KL}(q_\\phi(z|x) || p(x|z))\\geq 0.} \\\\ &amp;\\geq \\mathbb{E}_{q_\\phi(z|x)}[\\log p(z) - \\log q_\\phi(z|x) + \\log p(x|z) ] \\equiv \\mathbb{E}_{q_\\phi(z|x)}[\\log \\frac{p(x,z)}{q_\\phi(z|x)}] \\end{aligned}\\] The last line of the equations is the expression of the so-called Evidence Lower Bound (ELBO). Most literature online prefers the form in tight fraction. The name Evidence comes again from the chain rule of probability: . \\[\\underbrace{p(x)}_{\\text{evidence}} = \\frac{ \\underbrace{p(x, z)}_{\\text{joint probability}} }{ \\underbrace{p(z|x)}_{\\text{posterior}} }.\\] We have developed the approaches of estimating the probability of the observed data and of comparing them with the true probability distribution in this framework all along. And . \\[\\log p(x) \\geq \\mathbb{E}_{q_\\phi(z|x)}[\\log p(z) - \\log q_\\phi(z|x) + \\log p(x|z) ] \\equiv \\mathbb{E}_{q_\\phi(z|x)}[\\log \\frac{p(x,z)}{q_\\phi(z|x)}]\\] gives the name to Lower Bound. We now use ELBO to be the measure so that the corresponding value of observing those \\(x\\) in modeled distributions can be lower than that in the true distribution just because the modeled distribution is not close to the true distribution. The inequality about ELBO holds with a true \\(p(x|z)\\) being known, which is actually never the case in real life. A straightforward method to get the true distribution is to guess all the possible distribution to see which one provides the highest ELBO, but this is surely unfeasible. ELBO is now completely practicable, except for the numerator \\(p(x,z)\\) that can be further decomposed as \\(p(z)p_\\theta(x|z)\\) where the \\(_\\theta\\) indicates the modeling effort in decoder. However, it should be noted that the decomposition brings in approximation error via the modeling \\(\\theta\\), so we have in effect: . \\[\\mathbb{E}_{q_\\phi(z|x)}[\\log \\frac{p(x,z)}{q_\\phi(z|x)}] \\sim \\mathbb{E}_{q_\\phi(z|x)}[\\log \\frac{p(z)p_\\theta(x|z)}{q_\\phi(z|x)}]\\] I put \\(\\sim\\) there instead of equal sign. People explore the hypothesizing structure of latents within the encoder-decoder methodology, hoping that by poking around the unknown universe of the mechanism of the true distribution of the observed data \\(x\\), some opportunities of improving tractability can be created in terms of modeling. That is what we will discuss in the next chapter. After all, we are relieved to know that our focus has by far transferred from the incomputable \\(\\log p(x)\\) to the promising proxy objective ELBO now. ",
    "url": "/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/02-chpt2.html",
    
    "relUrl": "/02-chpt2.html"
  },"3": {
    "doc": "Chapter 3 - Dissecting the ELBO - Diffusion Models as Distribution-Transitioned Dynamics",
    "title": "Chapter 3 - Dissecting the ELBO - Diffusion Models as Distribution-Transitioned Dynamics",
    "content": "Chapter 3 - Dissecting the ELBO - Diffusion Models as Distribution-Transitioned Dynamics. From previous chapters, we understand that our primary objective is to model the true underlying data distribution \\(p_{data}(x)\\) of the observed data \\(x\\). A successful model must assign high probability to the observed samples, ensuring that the learned distribution captures the essential characteristics of the data. However, designing such a model requires careful consideration of the underlying structure of the data distribution: we believe that with a structure closer to real situation (that we imagine), the better we have a good model. As established in Chapter 1, natural images—despite their high-dimensional pixel representation—typically reside on a low-dimensional manifold. This observation motivates the introduction of latent variables \\(z\\), which encode the intrinsic low-dimensional structure of the data (as discussed in Chapter 2). By incorporating such latent variables, we aim to build a model that better reflects the true generative process of natural images. A well-structured model should generalize beyond the training data, avoiding the pitfall of overfitting. Without proper regularization or structural assumptions, the model could degenerate into a mixture of Dirac delta functions centered at the observed data points, i.e., \\(p(x) = \\frac{1}{N}\\sum^{N}_{i=1}\\delta(x-x_i)\\). Such a solution, while achieving perfect training likelihood, fails to capture meaningful structure and generalizes poorly to unseen data. We revisit our evidence: . \\[\\mathbb{E}_{q_\\phi(z|x)}[\\log \\frac{p(z)p_\\theta(x|z)}{q_\\phi(z|x)}]\\] From Chapter 2, we came to know that during the estimation of the evidence \\(p(x)\\), the prior \\(p(z)\\) is playing important role there, as the entire generative process begins with sampling from this distribution. This sampling operation is not merely a computational step, but rather a defining characteristic of modern generative models. When using a trained model, each sample \\(z \\sim p(z)\\) drawn from the latent space (typically encoded by the model’s encoder component) generates a corresponding output \\(x = g_\\theta(z)\\) through the decoder. The critical nature of sampling has driven significant developments in generative modeling, with most subsequent advances focusing on enhancing sampling efficiency, and improving the structure of the latent space. Many modern model architectures prioritize these sampling-related capabilities, often at the expense of less critical properties. For the time being, as for the prior \\(p(z)\\), sampling from noise \\(z \\sim \\mathcal{N}(0, I)\\) gives diverse starting points for generation. Even small perturbations in the noise direction create completely different images, amplifying diversity in generations. It has to start with a simple distribution followed by transformation decided by parameters \\(\\theta\\) where any complicate things can be stuffed in it. Then we can manipulate \\(p(x)\\) with all the simple mathematical tricks through decoder \\(p_\\theta(x|z)\\) and decoder \\(q_\\phi(z|x)\\), as we have done in previous chapter that led us to deal with a more tractable objective for maximization, i.e. ELBO, rather than the \\(p(x)\\) itself. Now, a critical question for now is: what should those encoder and decoder models be like? . We know that, to train any network, it requires supervised dataset. For either of the two networks, there is always one end that is missing data: the output for the encoder, and the input for the decoder.– we just don’t know what the latent is like in effect. Or, how should we expect them be like? . Here comes the deus ex machina idea for this chapter： . Consider a stochastic process \\(x_t \\in \\mathbb{R}^d\\) evolving according to the Itô stochastic differential equation (SDE): \\(dx_t = \\mu(x_t)dt + \\sigma(x_t)dW_t\\) where \\(W_t\\) is standard Brownian motion. The Fokker-Planck equation governs how the probability density \\(q(x,t)\\) evolves: . \\[\\frac{\\partial q(x,t)}{\\partial t} = -\\nabla_x \\cdot \\big[\\mu(x)q(x,t)\\big] + \\frac{1}{2}\\nabla_x^2 \\big[\\sigma^2(x)q(x,t)\\big]\\] with which we can set the prior to be the distribution convenient for us to sample, and we arrange the encoder/decoder to do the role the transition between the true data distribution and prior distribution. This seems so magic! but let’s have adequate faith in it because that is founded upon concrete mathematical deduction in the real of Stochastic differential equation. Before examining diffusion models, we first analyze GANs—a framework that transforms simple distributions into complex data distributions. GANs achieve their objective by progressively morphing a basic latent distribution (e.g., Gaussian or uniform) into the target data distribution during sampling. The generator network implements this through a parameterized nonlinear transformation, approximating the unknown target distribution. This approach is theoretically supported by the Kolmogorov-Arnold representation theorem, which guarantees that any continuous multivariate function can be decomposed into univariate functions. For high-dimensional spaces relevant to deep learning, the generator instantiates a practical realization of such hierarchical function composition, with each neural network layer implementing successive nonlinear transformations that collectively deform the input noise distribution. Back to our case. We are now deploying Itô stochastic differential equation approach to transition the distribution, by first designing the SDE for target distribution, let’s say \\(p(x)\\). Suppose we want the stationary distribution \\(q_{\\text{stationary}}(x)\\) to be the \\(p(x)\\). Let’s make an inspired choice by comparing: . \\[\\begin{align} \\mu(x) &amp;= \\frac{1}{2}\\nabla_x \\log p(x) \\\\ \\sigma(x) &amp;= 1 \\end{align}\\] Substituting into Fokker-Planck equation gives: . \\[\\frac{\\partial q(x,t)}{\\partial t} = -\\nabla_x \\cdot \\left[q(x,t) \\frac{1}{2}\\nabla_x \\log p(x)\\right] + \\frac{1}{2}\\nabla_x^2 q(x,t)\\] Here \\(q(x, t)\\) is the density function of \\(x_t\\). The equation induces a family of distributions \\(q(x, t)\\) describes where the stochastic process is likely to be at time \\(t\\). Dissolve the derivative of logarithm, we have: . \\[\\frac{\\partial q(x,t)}{\\partial t} = -\\nabla_x \\cdot \\left[q(x,t) \\frac{1}{2}\\frac{\\nabla_x p(x)}{p(x)}\\right] + \\frac{1}{2}\\nabla_x^2 q(x,t)\\] Since a stationary distribution \\(q_{stationary}(x)\\) satisfies . \\[\\frac{\\partial q(x,t)}{\\partial t} = 0\\] for all \\(x\\), we set the time derivative to zero: . \\[0 = -\\nabla_x \\cdot \\left[q_{stationary}(x) \\frac{1}{2}\\frac{\\nabla_x p(x)}{p(x)}\\right] + \\frac{1}{2}\\nabla_x^2 q_{stationary}(x)\\] The previous configuration of \\(\\mu(x)\\) and \\(\\sigma(x)\\) led us here where \\(p(x)\\) is indeed the \\(q_{stationary}\\) , the stationary distribution of the \\(q(x,t)\\) distribution, albeit they are not the same in their initial phase. To verify, we substitute \\(q_{stationary}(x) = p(x)\\) into this equation: \\(\\begin{align*} 0 &amp;= -\\nabla_x \\cdot \\left[p(x) \\frac{1}{2}\\frac{\\nabla_x p(x)}{p(x)}\\right] + \\frac{1}{2}\\nabla_x^2 p(x) \\\\ \\implies 0 &amp;= -\\nabla_x \\cdot \\left[\\frac{1}{2}\\nabla_x p(x)\\right] + \\frac{1}{2}\\nabla_x^2 p(x) \\\\ \\implies 0 &amp;= -\\frac{1}{2}\\nabla_x^2 p(x) + \\frac{1}{2}\\nabla_x^2 p(x) \\\\ \\implies 0 &amp;= 0 \\end{align*}\\) . Substitute \\(\\mu(x)\\) and \\(\\sigma(x)\\) again into the general Itô form: . \\[dx_t = \\underbrace{\\frac{1}{2}\\nabla_x \\log p(x_t)}_{\\text{Drift}}dt + \\underbrace{dW_t}_{\\text{Diffusion}}\\] This is called the Langevin equation, (one of) the soul(s) of diffusion model. The drift term resembles pushing particles toward regions where is larger. The diffusion term obviously gives its name to the diffusion model. It is noted that the \\(\\mu(x)\\) and \\(\\sigma(x)\\) can be multiplied with some variables constant against \\(x\\), while the converging to Gaussian still holds: . \\[\\begin{align} \\mu(x) &amp;= \\beta(t) \\frac{1}{2}\\nabla_x \\log p(x) \\\\ \\sigma(x) &amp;= \\sqrt{\\beta(t)} \\end{align}\\] which can verified by yourself. The choice \\(\\beta(t)\\) is preparatorial for the noise schedule in much of literature in community. It appears that it is the \\(\\nabla_x \\log p(x_t)\\) in the drift term that guides the process toward the target distribution \\(p(x)\\) after getting the stationary distribution \\(q_{stationary}(x)\\). For numerical implementation, choose step size \\(\\epsilon \\approx dt &gt; 0\\) to discretize other terms in the Langevin equation: . \\[dx_t \\approx \\Delta x = x_{t+1} - x_t\\] We consider \\(dW_t\\) now. For a small time step \\(\\epsilon\\), the increment is defined in Wiener process to follow a normal distribution: \\(W_{t+\\epsilon} - W_t \\sim \\mathcal{N}(0, \\epsilon).\\) In the infinitesimal limit, this becomes: \\(dW_t \\sim \\mathcal{N}(0, dt).\\) We have: \\(dW_t \\approx \\Delta W_t = W_{t+\\epsilon} - W_t.\\) Since \\(\\Delta W_t \\sim \\mathcal{N}(0, \\epsilon)\\), we can express it as: \\(\\Delta W_t = \\sqrt{\\epsilon} \\, z_t, \\quad z_t \\sim \\mathcal{N}(0, 1).\\) Therefore, . \\[\\mathcal{N}(0,\\Delta t) = \\sqrt{\\Delta t}\\,\\mathcal{N}(0,1)\\] Since: \\(\\Delta W_t \\sim \\mathcal{N}(0, \\Delta t).\\) the second moment (variance, since the mean is 0) is: \\(\\mathbb{E}[(\\Delta W_t)^2] = \\text{Var}(\\Delta W_t) = \\Delta t.\\) . We can now yield the Langevin update rule: . \\[x_{t+1} = x_t + \\frac{\\epsilon}{2}\\nabla_x \\log p(x_t) + \\sqrt{\\epsilon} z_t, \\quad z_t \\sim \\mathcal{N}(0, \\mathbf{I})\\] The Langevin dynamics equation is a fundamental technique for generating samples from distributions when we know their score function. The original Langevin dynamics is deductive as the consequence of physics and mathematics. However, it does not harm to be seen as an empirical formula. The diffusion model we’re discussing around here is accomplished via distribution transition between gaussian distribution to and from true data distribution. But, what target distribution should \\(p(x)\\) be? . Let’s take another look at the discretized Langevin equation: . \\[x_{t+1} = x_t + \\frac{\\epsilon_t}{2} \\nabla_x \\log p(x_t) + \\sqrt{\\epsilon_t} z_t, \\quad z_t \\sim \\mathcal{N}(0, \\mathbf{I}).\\] Over \\(T\\) steps, the cumulative transformation of \\(x_0\\) can be expressed as: . \\[x_T = x_0 + \\sum_{t=0}^{T-1} \\left( \\frac{\\epsilon_t}{2} \\nabla_x \\log p(x_t) \\right) + \\sum_{t=0}^{T-1} \\sqrt{\\epsilon_t} z_t.\\] In the first term, \\(\\sum_{t=0}^{T-1} \\frac{\\epsilon_t}{2} \\nabla_x \\log p(x_t)\\), if \\(\\epsilon_t \\to 0\\) sufficiently slowly, this term remains bounded but does not dominate the long-term behavior. The second term, \\(\\sum_{t=0}^{T-1} \\sqrt{\\epsilon_t} z_t\\), is a sum of independent, zero-mean Gaussian random variables with covariance \\(\\sum_{t=0}^{T-1} \\epsilon_t \\mathbf{I}\\). Here’s a second deus ex machina in this chapter: . Central Limit Theorem - given a sufficiently large sample size \\(n\\), the sampling distribution of the mean of any independent, identically distributed (i.i.d.) random variables will approximate a normal distribution \\(\\mathcal{N}(\\mu, \\sigma^2/n)\\), regardless of the original population’s distribution. Formally, if \\(X_1, X_2, \\dots, X_n\\) are i.i.d. with mean \\(\\mu\\) and variance \\(\\sigma^2\\), the sample mean \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) converges in distribution to a normal random variable: \\(\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2).\\) It can be used to justify the ubiquity of the normal distribution in statistics, especially when the underlying data is non-normal. By the Central Limit Theorem, in our Langevin equation, the distribution of this sum will converge to a Gaussian. And if \\(\\epsilon_t\\) is chosen such that \\(\\sum_{t=0}^{T-1} \\epsilon_t\\) grows while \\(\\epsilon_t \\to 0\\), then the distribution of this sum will converge to \\(\\mathcal{N}(0, \\sigma^2 \\mathbf{I})\\), where \\(\\sigma^2 = 2\\sum_{t=0}^{T-1} \\epsilon_t\\). Thus, as \\(T \\to \\infty\\), the drift term’s influence becomes negligible relative to the noise, and \\(x_T\\) behaves like a Gaussian random variable shifted by a finite drift. Properly normalizing \\(\\epsilon_t\\) by ensuring \\(\\sigma^2 = 1\\) leads to: \\(x_T \\approx \\mathcal{N}(0, \\mathbf{I}),\\) which independent of the initial \\(x_0\\). This emerges naturally from the noise accumulation process, justifying the choice of a Gaussian as the terminal distribution for the reverse diffusion process. The Gaussian is not an arbitrary assumption, but the inevitable limiting distribution under the given SDE dynamics. That’s also why we didn’t put the Gaussian prior as a deux ex machina thing: we simply said that we wanted a distribution to sample to be of maximum entropy given fixed mean and variance (cf. Chapter 1). Jubilation aside, how about we break it down in the context of Itô SDEs and the Fokker-Planck equation to see if uniform distribution is suitable for the job. Uniform distribution is not closed under additive noise. Adding noise (e.g. gaussian noise) to a uniform distribution also doesn’t give another uniform distribution, because the process would quickly become non-uniform and less analytically tractable. And, the score \\(\\nabla_x \\log p_t(x)\\) for uniform distributions is either undefined (flat probability density function between boundaries), or discontinuous (undefined at boundaries). The tractability of Gaussian distribution in generative models can be attributed to several properties. Among its many remarkable properties, the stability under addition stands out. Let us consider two independent Gaussian random variables, \\(X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)\\) and \\(Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y^2)\\), with probability density functions (PDFs) given respectively by: . \\[f_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma_X^2}} \\exp\\left( -\\frac{(x - \\mu_X)^2}{2\\sigma_X^2} \\right),\\] and . \\[f_Y(y) = \\frac{1}{\\sqrt{2\\pi \\sigma_Y^2}} \\exp\\left( -\\frac{(y - \\mu_Y)^2}{2\\sigma_Y^2} \\right).\\] Our object of study is the sum \\(Z = X + Y\\), whose distribution we shall derive explicitly. By the independence of \\(X\\) and \\(Y\\), the PDF of \\(Z\\) is given by the convolution of \\(f_X\\) and \\(f_Y\\): . \\[f_Z(z) = \\int_{-\\infty}^{\\infty} f_X(x) f_Y(z - x) \\, dx.\\] Substituting the expressions for \\(f_X\\) and \\(f_Y\\), we obtain: . \\[f_Z(z) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{(x - \\mu_X)^2}{2\\sigma_X^2} - \\frac{(z - x - \\mu_Y)^2}{2\\sigma_Y^2} \\right) \\, dx.\\] The crux of the proof lies in the simplification of the exponent. Let’s denote the argument of the exponential as: . \\[E(x) = \\frac{(x - \\mu_X)^2}{2\\sigma_X^2} + \\frac{(z - x - \\mu_Y)^2}{2\\sigma_Y^2}.\\] Our task is to express \\(E(x)\\) in the form \\(A(x - B)^2 + C\\), where \\(A, B\\) and \\(C\\) are quantities independent of \\(x\\). Expanding the squares, we have: . \\[E(x) = \\frac{(x^2 - 2\\mu_X x + \\mu_X^2)}{2\\sigma_X^2} + \\frac{(x^2 - 2x(z - \\mu_Y) + (z - \\mu_Y)^2)}{2\\sigma_Y^2}.\\] Collecting like terms in \\(x\\), we rewrite \\(E(x)\\) as: . \\[E(x) = \\left( \\frac{1}{2\\sigma_X^2} + \\frac{1}{2\\sigma_Y^2} \\right) x^2 - \\left( \\frac{\\mu_X}{\\sigma_X^2} + \\frac{z - \\mu_Y}{\\sigma_Y^2} \\right) x + \\left( \\frac{\\mu_X^2}{2\\sigma_X^2} + \\frac{(z - \\mu_Y)^2}{2\\sigma_Y^2} \\right).\\] Define: . \\[\\alpha = \\frac{1}{\\sigma_X^2} + \\frac{1}{\\sigma_Y^2}, \\quad \\beta = \\frac{\\mu_X}{\\sigma_X^2} + \\frac{z - \\mu_Y}{\\sigma_Y^2}, \\quad \\gamma = \\frac{\\mu_X^2}{2\\sigma_X^2} + \\frac{(z - \\mu_Y)^2}{2\\sigma_Y^2}.\\] Then, \\(E(x)\\) may be expressed as: . \\[E(x) = \\frac{\\alpha}{2} x^2 - \\beta x + \\gamma.\\] Completing the square in \\(x\\), we write: . \\[E(x) = \\frac{\\alpha}{2} \\left( x^2 - \\frac{2\\beta}{\\alpha} x \\right) + \\gamma = \\frac{\\alpha}{2} \\left( x - \\frac{\\beta}{\\alpha} \\right)^2 - \\frac{\\beta^2}{2\\alpha} + \\gamma.\\] Thus, the exponent decomposes into a perfect square in \\(x\\) and a residual term independent of \\(x\\). Substituting this back into the expression for \\(f_Z(z)\\), we have: . \\[f_Z(z) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y} \\exp\\left( -\\frac{\\beta^2}{2\\alpha} + \\gamma \\right) \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{\\alpha}{2} \\left( x - \\frac{\\beta}{\\alpha} \\right)^2 \\right) \\, dx.\\] The integral is now recognizable as the integral of a Gaussian function. Specifically, . \\[\\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{\\alpha}{2} \\left( x - \\frac{\\beta}{\\alpha} \\right)^2 \\right) \\, dx = \\sqrt{\\frac{2\\pi}{\\alpha}},\\] since the integrand is proportional to the PDF of a normal distribution with mean \\(\\frac{\\beta}{\\alpha}\\) and variance \\(\\frac{1}{\\alpha}\\). Thus, . \\[f_Z(z) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y} \\sqrt{\\frac{2\\pi}{\\alpha}} \\exp\\left( -\\frac{\\beta^2}{2\\alpha} + \\gamma \\right).\\] We now turn our attention to the exponent \\(-\\frac{\\beta^2}{2\\alpha} + \\gamma\\). Substituting back the definitions of \\(\\alpha, \\beta\\) and \\(\\gamma\\), we have: . \\[-\\frac{\\beta^2}{2\\alpha} + \\gamma = -\\frac{1}{2\\alpha} \\left( \\frac{\\mu_X}{\\sigma_X^2} + \\frac{z - \\mu_Y}{\\sigma_Y^2} \\right)^2 + \\frac{\\mu_X^2}{2\\sigma_X^2} + \\frac{(z - \\mu_Y)^2}{2\\sigma_Y^2}.\\] This expression, though cumbersome, may be simplified by algebraic manipulation. Let us denote \\(\\sigma_Z^2 = \\sigma_X^2 + \\sigma_Y^2\\), and observe that \\(\\alpha = \\frac{\\sigma_Z^2}{\\sigma_X^2 \\sigma_Y^2}\\). Then, . \\[-\\frac{\\beta^2}{2\\alpha} + \\gamma = -\\frac{\\sigma_X^2 \\sigma_Y^2}{2\\sigma_Z^2} \\left( \\frac{\\mu_X \\sigma_Y^2 + (z - \\mu_Y)\\sigma_X^2}{\\sigma_X^2 \\sigma_Y^2} \\right)^2 + \\frac{\\mu_X^2 \\sigma_Y^2 + (z - \\mu_Y)^2 \\sigma_X^2}{2\\sigma_X^2 \\sigma_Y^2}.\\] Simplifying, we find: . \\[-\\frac{\\beta^2}{2\\alpha} + \\gamma = -\\frac{(\\mu_X \\sigma_Y^2 + (z - \\mu_Y)\\sigma_X^2)^2}{2\\sigma_X^2 \\sigma_Y^2 \\sigma_Z^2} + \\frac{\\mu_X^2 \\sigma_Y^2 + (z - \\mu_Y)^2 \\sigma_X^2}{2\\sigma_X^2 \\sigma_Y^2}.\\] Factoring the numerator of the first term, we obtain: . \\[\\mu_X^2 \\sigma_Y^4 + 2\\mu_X (z - \\mu_Y) \\sigma_X^2 \\sigma_Y^2 + (z - \\mu_Y)^2 \\sigma_X^4.\\] Thus, . \\[-\\frac{\\beta^2}{2\\alpha} + \\gamma = \\frac{-( \\mu_X^2 \\sigma_Y^4 + 2\\mu_X (z - \\mu_Y) \\sigma_X^2 \\sigma_Y^2 + (z - \\mu_Y)^2 \\sigma_X^4 ) + \\mu_X^2 \\sigma_Y^2 \\sigma_Z^2 + (z - \\mu_Y)^2 \\sigma_X^2 \\sigma_Z^2}{2\\sigma_X^2 \\sigma_Y^2 \\sigma_Z^2}.\\] Substituting \\(\\sigma_Z^2 = \\sigma_X^2 + \\sigma_Y^2\\), the expression reduces, after cancellation, to: . \\[-\\frac{(z - (\\mu_X + \\mu_Y))^2}{2\\sigma_Z^2}.\\] Thus, the PDF of \\(Z\\) simplifies to: . \\[f_Z(z) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y} \\sqrt{\\frac{2\\pi \\sigma_X^2 \\sigma_Y^2}{\\sigma_Z^2}} \\exp\\left( -\\frac{(z - (\\mu_X + \\mu_Y))^2}{2\\sigma_Z^2} \\right).\\] Further simplification yields: . \\[f_Z(z) = \\frac{1}{\\sqrt{2\\pi \\sigma_Z^2}} \\exp\\left( -\\frac{(z - (\\mu_X + \\mu_Y))^2}{2\\sigma_Z^2} \\right).\\] This is precisely the PDF of a Gaussian random variable \\(Z \\sim \\mathcal{N}(\\mu_X + \\mu_Y, \\sigma_X^2 + \\sigma_Y^2)\\). We have thus demonstrated, through direct computation of the convolution integral, that the sum of two independent Gaussian random variables is itself Gaussian, with mean and variance given by the sums of the respective means and variances. Similarly, it is not hard to imagine that if \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then scaling by a constant \\(a\\) yields another Gaussian: . \\[aX \\sim \\mathcal{N}(a\\mu, a^2\\sigma^2).\\] Combining the addivity properties, let \\(X_1, X_2, \\dots, X_n\\) be independent Gaussian random variables, where \\(X_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\), and let \\(a_1, a_2, \\dots, a_n\\) be arbitrary constants. Then, the linear combination: . \\[Y = a_1 X_1 + a_2 X_2 + \\dots + a_n X_n\\] is still Gaussian, with mean and variance: . \\[Y \\sim \\mathcal{N}\\left( \\sum_{i=1}^n a_i \\mu_i, \\sum_{i=1}^n a_i^2 \\sigma_i^2 \\right).\\] which is the linearity of gaussian distribution. There are another two important features of Gaussian distribution. First, in Bayesian inference, if the prior and likelihood are Gaussian, the posterior is also Gaussian. We can see that by using Bayesian : . \\[p(\\theta | x) \\propto p(x | \\theta) \\times p(\\theta).\\] in substituting the expressions for the likelihood and the prior: . \\[p(\\theta | x) \\propto \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(x-\\theta)^2}{2\\sigma^2} \\right) \\times \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp \\left( -\\frac{(\\theta-\\mu_0)^2}{2\\sigma_0^2} \\right)\\] Combining the constants and the exponents, we thus have: . \\[p(\\theta | x) \\propto \\exp \\left( -\\frac{(x-\\theta)^2}{2\\sigma^2} - \\frac{(\\theta-\\mu_0)^2}{2\\sigma_0^2} \\right)\\] Second, the Gaussian PDF is infinitely differentiable, making it suitable for gradient-based optimization (e.g., the backpropagation in neural networks). For \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), the PDF is: . \\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\] The function \\(f(x)\\) consists of a polynomial term \\((x-\\mu)\\) (infinitely differentiable) and an exponential term \\(e^{-g(x)}\\) where \\(g(x) = \\frac{(x-\\mu)^2}{2\\sigma^2}\\) (infinitely differentiable). By the chain rule, the \\(n\\)-th derivative exists for all \\(n \\in \\mathbb{N}\\): . \\[f^{(n)}(x) = \\frac{d^n}{dx^n} \\left[ \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\right]\\] This yields a product of polynomials in \\((x-\\mu)\\) with the original exponential that is always preserved. We reiterate the soul equation of Langevin dynamics here: . \\[x_{t+1} = x_t + \\frac{\\epsilon}{2}\\nabla_x \\log p(x_t) + \\sqrt{\\epsilon} z_t\\] We check this in the background of sampling/noising via distribution transition perspective. The equation describes a discrete-time stochastic process where \\(x_{t+1}\\) is obtained from \\(x_t\\)by taking a small step in the direction of the gradient of the log-probability \\(\\nabla_x \\log p(x_t)\\), scaled by a step size \\(\\epsilon/2\\), and perturbed by Gaussian noise \\(\\sqrt{\\epsilon} z_t\\), where \\(z_t \\sim \\mathcal{N}(0, I)\\). If the drift term is zero and only the noise term contributes, this will lead to a pure diffusion process that simply adds noise, where the data is gradually corrupted over a sequence of time steps toward a gaussian. Conversely, using \\(\\nabla_x \\log p(x_t)\\)—the gradient of the log-probability of the data at each noise level, the equation can be guided with the direction in which the probability density increases most steeply, effectively “pulling” samples back toward the true data distribution. Thus, the Langevin equation provides a principled way to understand how iterative noise addition and denoising enable the generation of complex data distributions from simple noise. Each denoising step must consider directional accuracy, because directions matter more than norms in high-dim spaces (since everything has similar norm). The direction of the noise vector and how it’s removed is critical. The model’s job is to steer the vector toward the data manifold, starting from nearly orthogonal directions. The quantity in question is the gradient of the log of true density of data, w.r.t the data variable, which is now referred to as score: . \\[\\nabla_x\\log q_{data}(x)\\triangleq s(x).\\] In this spirit, now we take another look at the tensors in the high-dimensional space that locates the gaussian as our target prior distribution therein: how are they correlated in terms of direction? Let \\(\\mathbf{X} \\in \\mathbb{R}^d\\) be a random vector with i.i.d. entries \\(X_i \\sim \\mathcal{N}(0, 1)\\). Then \\(\\mathbf{X} \\sim \\mathcal{N}(0, I_d)\\), and the Euclidean norm \\(\\|\\mathbf{X}\\|_2 ^2 = \\sum_{i=1}^d X_i^2 \\sim \\chi^2 (d)\\) by definition. As \\(d \\to \\infty\\), the norm concentrates \\(|\\mathbf{X}|_2 \\approx \\sqrt{d}\\) with high probability, as we have discussed in Chapter 1. We are now checking the distribution of \\(\\mathbf{X}/|\\mathbf{X}|_2\\) , i.e. the directional unit vectors. We start considering an orthogonal matrix \\(Q\\in O(d)\\). An orthogonal matrix satisfies \\(Q^TQ=QQ^T=I_d\\)​. It preserves lengths and angles. When we apply \\(Q\\) to \\(\\mathbf{X}\\), the transformed vector \\(Q\\mathbf{X}\\) is also normally distributed because we have proved above that linear transformations of Gaussians remain Gaussian. Specifically, for any linear transformation \\(A\\), \\(A\\mathbf{X}\\sim \\mathcal{N}(A\\mu,A\\Sigma A^T)\\). Here, \\(X\\) has mean 0 and covariance \\(I_d\\)​, so \\(QX\\) has mean \\(Q⋅0=0\\) and covariance \\(QI_d​Q^T=QQ^T=I_d\\)​. Thus, \\(Q\\mathbf{X}\\sim \\mathcal{N}(0,I_d)\\), proving invariance under orthogonal transformations, i.e. \\[Q \\mathbf{X} \\sim \\mathbf{X} \\quad \\text{for any } Q \\in O(d)j.\\] This implies that \\(Q\\) preserves lengths (i.e., \\(|Q\\mathbf{x}| = |\\mathbf{x}|\\)) and angles in rotations and reflections in \\(d\\)-dimensional space. Thus, the direction \\(\\mathbf{U} = \\mathbf{X}/|\\mathbf{X}|_2\\) must be rotationally invariant on the sphere (rotation is one type of orthogonal transformation), which implies: . \\[\\mathbf{U} \\sim \\text{Unif}(S^{d-1}),\\] where \\(\\text{Unif}\\) means uniform distribution. This is because if a distribution is rotationally invariant, then the probability measure doesn’t prefer any direction, i.e., all directions must be equally likely, due to \\(Q \\mathbf{X} \\sim \\mathbf{X}\\). With high probability, any two independent samples in high dimensions lie nearly orthogonal, which means the angle between two random vectors on \\(S^{d-1}\\) tends toward \\(\\pi/2\\) as \\(d \\to \\infty\\). Let’s thus calculate the inner product of any two directions. For \\(\\mathbf{u}, \\mathbf{v} \\sim \\text{Unif}(S^{d-1})\\), we can have: . \\[\\mathbf{u} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}, \\quad \\mathbf{v} = \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|_2},\\] where \\(\\mathbf{x}, \\mathbf{y} \\sim \\mathcal{N}(0, I_d)\\) i.i.d. with each component \\(x_i, y_i \\sim \\mathcal{N}(0, 1)\\) i.i.d., and . \\[\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}.\\] Then consider: . \\[\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{i=1}^d x_i y_i\\] as a sum of \\(d\\) i.i.d. mean-0, variance-1 random variables. Since \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are independent: . \\[\\mathbb{E}[\\langle \\mathbf{x}, \\mathbf{y} \\rangle] = \\mathbb{E}\\left[\\sum_{i=1}^d x_i y_i\\right] = \\sum_{i=1}^d \\mathbb{E}[x_i y_i] = \\sum_{i=1}^d \\mathbb{E}[x_i]\\mathbb{E}[y_i] = \\sum_{i=1}^d 0 \\cdot 0 = 0\\] Again using independence: . \\[\\text{Var}[\\langle \\mathbf{x}, \\mathbf{y} \\rangle] = \\text{Var}\\left[\\sum_{i=1}^d x_i y_i\\right] = \\sum_{i=1}^d \\text{Var}[x_i y_i]\\] For each term \\(x_i y_i\\) where \\(x_i, y_i\\) are independent standard normal: . \\[\\text{Var}[x_i y_i] = \\mathbb{E}[x_i^2 y_i^2] - (\\mathbb{E}[x_i y_i])^2 = \\mathbb{E}[x_i^2]\\mathbb{E}[y_i^2] - 0 = 1 \\cdot 1 = 1\\] Therefore: . \\[\\text{Var}[\\langle \\mathbf{x}, \\mathbf{y} \\rangle] = \\sum_{i=1}^d 1 = d\\] Since each \\(x_i y_i\\) is a product of independent normal random variables, and we’re summing \\(d\\) such terms, the sum converges to a normal distribution. In this way, we have: . \\[\\langle \\mathbf{x}, \\mathbf{y} \\rangle \\sim \\mathcal{N}(0, d)\\] Using concentration bounds that we already know, we have: . \\[\\frac{\\|\\mathbf{x}\\|}{\\sqrt{d}} \\xrightarrow{P} 1, \\frac{\\|\\mathbf{y}\\|}{\\sqrt{d}} \\xrightarrow{P} 1\\] Now combine everything: . \\[\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\approx \\frac{\\mathcal{N}(0, d)}{\\sqrt{d} \\cdot \\sqrt{d}} = \\mathcal{N}\\left(0, \\frac{1}{d} \\right)\\] Hence: . \\[\\langle \\mathbf{u}, \\mathbf{v} \\rangle \\xrightarrow{P} 0 \\quad \\text{as } d \\to \\infty\\] The cosine of the angle between two random unit vectors converges in probability to 0, i.e.: . \\[\\theta(\\mathbf{u}, \\mathbf{v}) \\xrightarrow{P} \\frac{\\pi}{2}\\] . With so much illustration on the SDE data distribution transition on how to transform an unknown distribution into normal distribution. What about the reverse one? . We start by Itô stochastic differential equation (SDE) . \\[dx_t = \\mu(x_t)dt + \\sigma(x_t)dW_t\\] again, as well as: . \\[\\mathbf{x}_{t+\\varepsilon} \\approx \\mathbf{x}_t + \\mathbf{\\mu}(\\mathbf{x}_t, t)\\varepsilon + \\sigma(t)\\sqrt{\\varepsilon} \\cdot \\boldsymbol{z},\\quad \\text{where }z\\sim \\mathcal{N}(0,I).\\] So, in perspective of conditional probability, the transition density is approximately: . \\[p(\\mathbf{x}_{t+\\varepsilon} \\mid \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t+\\varepsilon};\\, \\mathbf{x}_t + \\mathbf{\\mu}(\\mathbf{x}_t, t)\\varepsilon,\\, \\sigma^2(t)\\varepsilon \\mathbf{I})\\] Now filp time, by going from \\(\\mathbf{x}_{t+\\varepsilon}\\) back to \\(\\mathbf{x}_t\\). Using Bayes’ rule: . \\[p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+\\varepsilon}) \\propto p(\\mathbf{x}_{t+\\varepsilon} \\mid \\mathbf{x}_t) \\cdot p_t(\\mathbf{x}_t)\\] The reverse-time drift should move us back toward areas of high probability, weighted by how likely it was to get there in the forward step. The transition density \\(p(\\mathbf{x}_{t+\\varepsilon} \\mid \\mathbf{x}_t)\\) can be perceived as a Gaussian centered at \\(\\mathbf{x}_t + \\mathbf{\\mu}(\\mathbf{x}_t, t)\\varepsilon\\) with covariance \\(\\sigma^2(t)\\varepsilon \\mathbf{I}\\): . \\[p(\\mathbf{x}_{t+\\varepsilon} \\mid \\mathbf{x}_t) \\propto \\exp\\left(-\\frac{\\|\\mathbf{x}_{t+\\varepsilon} - \\mathbf{x}_t - \\mathbf{\\mu}(\\mathbf{x}_t, t)\\varepsilon\\|^2}{2\\sigma^2(t)\\varepsilon}\\right).\\] Taking the logarithm (ignoring normalization constants): . \\[\\log p(\\mathbf{x}_{t+\\varepsilon} \\mid \\mathbf{x}_t) = -\\frac{\\|\\mathbf{x}_{t+\\varepsilon} - \\mathbf{x}_t - \\mathbf{\\mu}(\\mathbf{x}_t, t)\\varepsilon\\|^2}{2\\sigma^2(t)\\varepsilon} + \\text{const}.\\] Using Bayes’ rule, the reverse conditional density is: . \\[\\log p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+\\varepsilon}) = \\log p(\\mathbf{x}_{t+\\varepsilon} \\mid \\mathbf{x}_t) + \\log p_t(\\mathbf{x}_t) - \\log p_{t+\\varepsilon}(\\mathbf{x}_{t+\\varepsilon}).\\] For small \\(\\varepsilon\\), we approximate . \\[\\log p_{t+\\varepsilon}(\\mathbf{x}_{t+\\varepsilon}) \\approx \\log p_t(\\mathbf{x}_{t+\\varepsilon})\\] since the density evolves slowly, giving: . \\[\\log p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+\\varepsilon}) = \\log p(\\mathbf{x}_{t+\\varepsilon} \\mid \\mathbf{x}_t) + \\log p_t(\\mathbf{x}_t) - \\log p_t(\\mathbf{x}_{t+\\varepsilon}) + \\text{const}.\\] Substituting the Gaussian transition density: . \\[\\log p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+\\varepsilon}) = -\\frac{\\|\\mathbf{x}_{t+\\varepsilon} - \\mathbf{x}_t - \\mathbf{\\mu}(\\mathbf{x}_t, t)\\varepsilon\\|^2}{2\\sigma^2(t)\\varepsilon} + \\log p_t(\\mathbf{x}_t) - \\log p_t(\\mathbf{x}_{t+\\varepsilon}) + \\text{const}.\\] For small \\(\\varepsilon\\), \\(\\mathbf{x}_t\\) is close to \\(\\mathbf{x}_{t+\\varepsilon}\\), so we Taylor expand \\(\\log p_t(\\mathbf{x}_t)\\) around \\(\\mathbf{x}_{t+\\varepsilon}\\): . \\[\\log p_t(\\mathbf{x}_t) \\approx \\log p_t(\\mathbf{x}_{t+\\varepsilon}) + \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})^T (\\mathbf{x}_t - \\mathbf{x}_{t+\\varepsilon}) + \\mathcal{O}(\\|\\mathbf{x}_t - \\mathbf{x}_{t+\\varepsilon}\\|^2).\\] Substituting this back: . \\[\\log p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+\\varepsilon}) \\approx -\\frac{\\|\\mathbf{x}_{t+\\varepsilon} - \\mathbf{x}_t - \\mathbf{\\mu}(\\mathbf{x}_t, t)\\varepsilon\\|^2}{2\\sigma^2(t)\\varepsilon} + \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})^T (\\mathbf{x}_t - \\mathbf{x}_{t+\\varepsilon}) + \\text{const}.\\] We approximate . \\[\\mathbf{\\mu}(\\mathbf{x}_t, t) \\approx \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\] and rewrite: . \\[\\log p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+\\varepsilon}) \\approx -\\frac{\\|\\mathbf{x}_{t+\\varepsilon} - \\mathbf{x}_t - \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon\\|^2}{2\\sigma^2(t)\\varepsilon} + \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})^T (\\mathbf{x}_t - \\mathbf{x}_{t+\\varepsilon}) + \\text{const}.\\] Let . \\[\\mathbf{z} = \\mathbf{x}_t - \\mathbf{x}_{t+\\varepsilon} + \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon,\\] so the right-hand side is: . \\[-\\frac{\\|\\mathbf{z}\\|^2}{2\\sigma^2(t)\\varepsilon} + \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})^T \\mathbf{z} + \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})^T \\mathbf{\\mu(x_{t+\\epsilon},t)\\epsilon} .\\] where . \\[\\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})^T \\mathbf{\\mu(x_{t+\\epsilon},t)\\epsilon}\\] can be absorbed into constant. Then, make a square in form of something like \\((z - a)^2\\): . \\[-\\frac{\\|\\mathbf{z}\\|^2}{2\\sigma^2(t)\\varepsilon} + \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})^T \\mathbf{z} = -\\frac{1}{2\\sigma^2(t)\\varepsilon} \\left( \\|\\mathbf{z} - \\sigma^2(t)\\varepsilon \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})\\|^2 - \\sigma^4(t)\\varepsilon^2 \\|\\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})\\|^2 \\right).\\] The completed-square term is: . \\[\\|\\mathbf{z} - \\sigma^2(t)\\varepsilon \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})\\|^2.\\] Recall that \\(\\mathbf{z}\\) is defined as: . \\[\\mathbf{z} = \\mathbf{x}_{t+\\varepsilon} - \\mathbf{x}_t - \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon.\\] Substitute \\(\\mathbf{z}\\) back into the squared term: . \\[\\|\\mathbf{x}_{t+\\varepsilon} - \\mathbf{x}_t - \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon - \\sigma^2(t)\\varepsilon \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})\\|^2.\\] This can be rewritten as: . \\[\\|\\mathbf{x}_t - (\\mathbf{x}_{t+\\varepsilon} - \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon + \\sigma^2(t)\\varepsilon \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon}))\\|^2.\\] Now, plug this back into the original expression: . \\[-\\frac{1}{2\\sigma^2(t)\\varepsilon} \\left( \\|\\mathbf{x}_t - (\\mathbf{x}_{t+\\varepsilon} - \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon + \\sigma^2(t)\\varepsilon \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon}))\\|^2 - \\sigma^4(t)\\varepsilon^2 \\|\\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon})\\|^2 \\right).\\] Considering \\(\\epsilon \\rightarrow 0\\) , we can write: . \\[-\\frac{\\|\\mathbf{x}_t - (\\mathbf{x}_{t+\\varepsilon} - \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon + \\sigma^2(t)\\varepsilon \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon}))\\|^2}{2\\sigma^2(t)\\varepsilon}\\] This is the exponential part of the Gaussian \\(p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+\\varepsilon})\\). So its mean of the Gaussian is: . \\[\\mathbf{x}_{t+\\varepsilon} - \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon + \\sigma^2(t)\\varepsilon \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon}).\\] The dominant term is: . \\[\\log p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+\\varepsilon}) \\approx -\\frac{\\|\\mathbf{x}_t - (\\mathbf{x}_{t+\\varepsilon} + \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon - \\sigma^2(t)\\varepsilon \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon}))\\|^2}{2\\sigma^2(t)\\varepsilon} + \\text{const}.\\] Thus, the conditional mean of \\(\\mathbf{x}_t\\) given \\(\\mathbf{x}_{t+\\varepsilon}\\) is: . \\[\\mathbf{x}_t \\approx \\mathbf{x}_{t+\\varepsilon} - \\mathbf{\\mu}(\\mathbf{x}_{t+\\varepsilon}, t)\\varepsilon + \\sigma^2(t)\\varepsilon \\nabla \\log p_t(\\mathbf{x}_{t+\\varepsilon}).\\] The reverse drift is: . \\[\\tilde{\\mathbf{\\mu}}(\\mathbf{x}, t) = \\mathbf{\\mu}(\\mathbf{x}, t) - \\sigma(t)^2 \\nabla_{\\mathbf{x}} \\log p_t(\\mathbf{x})\\] Hence, . \\[d\\mathbf{x} = \\left[ \\mathbf{\\mu}(\\mathbf{x}, t) - \\sigma(t)^2 \\nabla_{\\mathbf{x}} \\log p_t(\\mathbf{x}) \\right] dt + \\sigma(t) d\\bar{\\mathbf{w}}_t\\] Considering the definition of mean and that of expectation, we sometimes confront situation with \\(\\mu(\\mathbf{x},t)=0\\) we can also have: . \\[\\mathbb{E}[x_t \\mid x_{t+\\epsilon}] = x_{t+\\epsilon} + \\sigma(t)^2 \\nabla_{x_t} \\log p(x_t)\\] Here the plus before \\(\\sigma(t)\\) is a denotation custom. This is Tweedie’s formula, a powerful, popular and elegant formula in community. If we refer to the formula of transforming unknown true data distribution to normal gaussian as noise adding, and the formula of reverse process as denoising, then the Tweedie’s formula beautifully shows optimal denoising (what we want) on the left side, and what we can compute (i.e. current position + score-based movement) on the right. It is noted that the situation of \\(\\mu(\\mathbf{x},t)=0\\) is a typical common practice for the community to add noise. Simply, at a point \\(x\\), it tell us the best direction to step into (with little step-size \\(\\delta\\)) if we would like to see a point \\(x’\\) with slightly higher likelihood . \\[x' = x + \\delta \\cdot \\nabla_x \\log q_{data}(x) \\big|_{x=x}\\] This is quite easy to imagine as this is a simple geometric description. But now, we will see that for a noisy observation \\(x_t\\), the optimal denoising direction is: . \\[\\mathbb{E}[x_0 | x_t] - x_t = \\sigma_t^2 \\cdot \\nabla_{x_t} \\log p(x_t)\\] which shows that denoising optimally is equivalent to moving in the score direction. This is the mathematical miracle that makes diffusion models possible: a simple regression loss secretly teaches the network to estimate probability gradients, which is exactly what we need for sampling from complex distributions. The best Bayesian denoiser (in expectation) of the noisy input is given by the input plus a term involving the score function of the noisy marginal. In essence, instead of directly maximizing \\(\\log p_{true}(x_{observed})\\) (which requires computing normalization constants), we can learn the geometry of the probability landscape through its gradients. Now, we are so glad to see ourselves capable of transitioning from an. unknown true data distribution, towards a general normal Gaussian, and retrieve it back. That’s exactly how we manipulate the observed data as machine learning training materials! Now that we have at hand a bunch of true images as dataset. Is it enough for us to start the training in diffusion model by comparing pixelwise difference from input and output images? Yes, but remember our goal is to maximize the data distribution. So we still need to go through one last block of the diffusion model theory — the loss function. By definition, the loss function should be . \\[L_{\\text{KL}} = \\mathbb{E}_{z \\sim q_{data}}\\left[ \\log q_{data}(x) - \\log p_\\theta(x) \\right].\\] However, since the distribution transition invovles mainly the score, we would like to express the loss function in terms of score. An intuitive loss function would therefore be like . \\[\\mathcal{L}_{score} = \\mathbb{E}_{x\\sim q_{data}}\\left[\\left|\\nabla_x \\log p_\\theta(x) - \\nabla_x \\log q_{data}(x)\\right|^2\\right]\\] i.e. an observation-weighted divergence of minimization aiming to match the gradients of the log-densities. Now we give it a more robust analysis. Let’s consider a parametric family of distributions \\(p_\\epsilon(x) = p(x)(1 + \\epsilon h(x))\\) that represents a small perturbation from \\(p(x)\\), where \\(h(x)\\) is a function with \\(\\mathbb{E}_p[h(x)] = 0\\) to ensure normalization. The perturbation as a technique is a popular way to bring gradient sort-of-thing onto the stage. The KL divergence between \\(p(x)\\) and \\(p_\\epsilon(x)\\) is: . \\[KL(p||p_\\epsilon) = \\mathbb{E}_p\\left[\\log \\frac{p(x)}{p(x)(1 + \\epsilon h(x))}\\right] = \\mathbb{E}_p[-\\log(1 + \\epsilon h(x))]\\] Using Taylor expansion for small \\(\\epsilon\\): . \\[-\\log(1 + \\epsilon h(x)) \\approx -\\epsilon h(x) + \\frac{\\epsilon^2 h(x)^2}{2} + O(\\epsilon^3)\\] So: . \\[KL(p||p_\\epsilon) \\approx -\\epsilon \\mathbb{E}_p[h(x)] + \\frac{\\epsilon^2}{2}\\mathbb{E}_p[h(x)^2] + O(\\epsilon^3)\\] Since \\(\\mathbb{E}_p[h(x)] = 0\\) by construction: . \\[KL(p||p_\\epsilon) \\approx \\frac{\\epsilon^2}{2}\\mathbb{E}_p[h(x)^2] + O(\\epsilon^3)\\] Now, for score-based perturbations, we can express \\(h(x)\\) in terms of the score function. When \\(p_\\epsilon\\) differs from \\(p\\) by a small change in the score function, it can be shown that: . \\[KL(p||p_\\epsilon) \\approx \\frac{\\epsilon^2}{2}\\mathbb{E}_p[||\\nabla_x \\log p(x) - \\nabla_x \\log p_\\epsilon(x)||^2] + O(\\epsilon^3)\\] The term inside the expectation is precisely the \\(L_{\\text{score}}\\) divergence, which is called Fisher divergence in the community. Fisher divergence can be seen as a first-order Taylor approximation to the KL divergence in the space of probability distributions, making it especially useful when exact likelihood is intractable. In the above, by admitting that the KL divergence is measuring the loss between a model and the true data distribution, we proved the equivalence of Fisher divergence with KL divergence. There is another proof illustrating that Fisher divergence itself is geared towards the finding of the true distribution from a model. Assume the score \\(s_{\\theta}(x) = \\nabla_x \\log p_{\\theta}(x)\\) is the gradient of the log-likelihood. Starting from . \\[J_{SM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\frac{1}{2} |s_\\theta(x) - \\nabla_x \\log q_{data}(x)|^2 \\right]\\] The squared norm can be expanded as: . \\[|s_\\theta(x) - \\nabla_x \\log q_{data}(x)|^2 = |s_\\theta(x)|^2 - 2s_\\theta(x)^T \\nabla_x \\log q_{data}(x) + |\\nabla_x \\log q_{data}(x)|^2\\] Therefore: . \\[J_{SM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\frac{1}{2} |s_\\theta(x)|^2 - s_\\theta(x)^T \\nabla_x \\log q_{data}(x) + \\frac{1}{2} |\\nabla_x \\log q_{data}(x)|^2 \\right]\\] Split it into: . \\[J_{SM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\frac{1}{2} |s_\\theta(x)|^2 \\right] - \\mathbb{E}_{x \\sim q_{data}(x)} \\left[ s_\\theta(x)^T \\nabla_x \\log q_{data}(x) \\right] + \\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\frac{1}{2} |\\nabla_x \\log q_{data}(x)|^2 \\right]\\] The third term depends only on the data distribution and not on \\(\\theta\\), so we have: . \\[\\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\frac{1}{2} |\\nabla_x \\log q_{data}(x)|^2 \\right] = \\text{const}\\] For the middle term, we have: . \\[\\mathbb{E}_{x \\sim q_{data}(x)} \\left[ s_\\theta(x)^T \\nabla_x \\log q_{data}(x) \\right] = \\int s_\\theta(x)^T \\nabla_x \\log q_{data}(x) \\cdot q_{data}(x) , dx\\] Since . \\[\\nabla_x \\log q_{data}(x) = \\frac{\\nabla_x q_{data}(x)}{q_{data}(x)}\\] , we have: . \\[\\mathbb{E}_{x \\sim q_{data}(x)} \\left[ s_\\theta(x)^T \\nabla_x \\log q_{data}(x) \\right] = \\int s_\\theta(x)^T \\nabla_x q_{data}(x) dx\\] Let \\(x \\in \\mathbb{R}^d\\) and \\(s_\\theta(x) \\triangleq [s_1(x), s_2(x), \\ldots, s_d(x)]^T\\) be a vector field. The integrand for the middle term now is: . \\[s_\\theta(x)^T \\nabla_x q_{data}(x) = \\sum_{i=1}^d s_i(x) \\frac{\\partial q_{data}(x)}{\\partial x_i}\\] For each component \\(i\\), we apply integral to the integrand and integrate by parts: . \\[\\int s_i(x) \\frac{\\partial q_{data}(x)}{\\partial x_i} dx = \\underbrace{s_i(x) q_{data}(x) \\Big|_{\\text{boundary}}}_{\\text{boundary term}} - \\int \\frac{\\partial s_i(x)}{\\partial x_i} q_{data}(x) dx\\] We assume the boundary term vanishes, because: . | Either the data distribution \\(q_{data}(x)\\) has compact support (zero outside a bounded region) | Or \\(q_{data}(x)\\) decays faster than \\(s_i(x)\\) grows as \\(|x| \\to \\infty\\) | Or we’re working on a torus/periodic domain Sum over all components: | . \\[\\sum_{i=1}^d \\int s_i(x) \\frac{\\partial q_{data}(x)}{\\partial x_i} dx = -\\sum_{i=1}^d \\int \\frac{\\partial s_i(x)}{\\partial x_i} q_{data}(x) dx\\] The sum . \\[\\sum_{i=1}^d \\frac{\\partial s_i(x)}{\\partial x_i}\\] is the divergence of the vector field \\(s_\\theta(x)\\): . \\[\\sum_{i=1}^d \\frac{\\partial s_i(x)}{\\partial x_i}= \\nabla_x \\cdot s_\\theta(x) = \\text{div}(s_\\theta) = \\text{tr}(\\nabla_x s_\\theta(x))\\] i.e. the trace of the Jacobian matrix of \\(s_\\theta(x)\\): \\(\\nabla_x \\mathbf{s}_\\theta(\\mathbf{x}) = \\begin{bmatrix} \\dfrac{\\partial s_1}{\\partial x_1} &amp; \\dfrac{\\partial s_1}{\\partial x_2} &amp; \\cdots &amp; \\dfrac{\\partial s_1}{\\partial x_d} \\\\ \\dfrac{\\partial s_2}{\\partial x_1} &amp; \\dfrac{\\partial s_2}{\\partial x_2} &amp; \\cdots &amp; \\dfrac{\\partial s_2}{\\partial x_d} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\dfrac{\\partial s_d}{\\partial x_1} &amp; \\dfrac{\\partial s_d}{\\partial x_2} &amp; \\cdots &amp; \\dfrac{\\partial s_d}{\\partial x_d} \\end{bmatrix}\\) . Therefore, the middle term is now: . \\[\\int s_\\theta(x)^T \\nabla_x q_{data}(x) dx = -\\int (\\nabla_x \\cdot s_\\theta(x)) q_{data}(x) dx = -\\int \\text{tr}(\\nabla_x s_\\theta(x)) q_{data}(x) , dx\\] Convert back to expectation form: . \\[\\int s_\\theta(x)^T \\nabla_x q_{data}(x) dx = -\\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] Substitute back . \\[J_{SM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\frac{1}{2} |s_\\theta(x)|^2 \\right] + \\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right] + \\text{const} = \\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) + \\frac{1}{2} |s_\\theta(x)|^2 \\right] + \\text{const}\\] When \\(s_\\theta(x) = \\nabla_x \\log q_{data}(x)\\), the objective becomes: . \\[J_{SM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\text{tr}(\\nabla_x (\\nabla_x \\log q_{data}(x))) + \\frac{1}{2} |\\nabla_x \\log q_{data}(x)|^2 \\right] + \\text{const}\\] The trace term is the Laplacian of the log-density: . \\[\\text{tr}(\\nabla_x (\\nabla_x \\log q_{data}(x))) = \\nabla_x^2 \\log q_{data}(x)\\] For the expectation of the Laplacian term: . \\[\\mathbb{E}_{x \\sim q_{data}(x)}[\\nabla_x^2 \\log q_{data}(x)] = \\int q_{data}(x) \\nabla_x^2 \\log q_{data}(x) dx\\] Using the identity \\(\\nabla_x^2 \\log q_{data}(x) = \\frac{\\nabla_x^2 q_{data}(x)}{q_{data}(x)} - \\frac{|\\nabla_x q_{data}(x)|^2}{q_{data}(x)^2}\\): . \\[= \\int \\nabla_x^2 q_{data}(x) dx - \\int \\frac{|\\nabla_x q_{data}(x)|^2}{q_{data}(x)} dx\\] The first integral equals zero (assuming proper boundary conditions), and the second integral is: . \\[-\\int q_{data}(x) |\\nabla_x \\log q_{data}(x)|^2 dx = -\\mathbb{E}_{x \\sim q_{data}(x)}[|\\nabla_x \\log q_{data}(x)|^2]\\] Substituting back: . \\(\\begin{aligned} &amp;\\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\nabla_x^2 \\log q_{data}(x) + \\frac{1}{2} \\|\\nabla_x \\log q_{data}(x)\\|^2 \\right] \\\\ &amp;= -\\mathbb{E}_{x \\sim q_{data}(x)}\\left[\\|\\nabla_x \\log q_{data}(x)\\|^2\\right] + \\frac{1}{2}\\mathbb{E}_{x \\sim q_{data}(x)}\\left[\\|\\nabla_x \\log q_{data}(x)\\|^2\\right] \\\\ &amp;= -\\frac{1}{2}\\mathbb{E}_{x \\sim q_{data}(x)}\\left[\\|\\nabla_x \\log q_{data}(x)\\|^2\\right] \\end{aligned}\\) For \\(J_{SM}(\\theta) = 0\\) when \\(s_\\theta(x) = \\nabla_x \\log q_{data}(x)\\), the constant must be: . \\[\\text{const} = \\frac{1}{2}\\mathbb{E}_{x \\sim q_{data}(x)}[|\\nabla_x \\log q_{data}(x)|^2]\\] With this choice of constant, \\(J_{SM}(\\theta) = 0\\) if and only if: . \\[\\mathbb{E}_{x \\sim q_{data}(x)} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) + \\frac{1}{2} |s_\\theta(x)|^2 \\right] = \\frac{1}{2}\\mathbb{E}_{x \\sim q_{data}(x)}[|\\nabla_x \\log q_{data}(x)|^2]\\] This equality holds precisely when \\(s_\\theta(x) = \\nabla_x \\log q_{data}(x)\\), proving that the score matching objective is zero only when \\(s_\\theta(x)\\) matches the true score function. Therefore, when you train a diffusion model by minimizing the denoising loss, you’re implicitly minimizing the Fisher divergence between your model’s score function and the true score function of the noisy data distribution. The above analysis is saying that when distributions are “close” in the space of probability distributions, the second-order approximation of KL divergence is proportional to the Fisher divergence. This relationship explains why minimizing Fisher divergence is a valid alternative to minimizing KL divergence—the former approximates the latter when distributions are similar, and both reach zero exactly when the distributions match. By shifting focus from “what is the density?” to “in which direction does density increase?”, these methods provide a practical way to learn complex distributions without explicit normalization, while still enabling high-quality sampling through Langevin dynamics or similar approaches. Score bridges geometry of data and denoising, which at least for me quite fascinating. Because, at first glance two operations in two realms: one in the geometry of data and the other in architecture of transformations. The rest of the works now goes to score prediction, as is usually called as score matching in community. Here’s how it works in different approaches. First, let’s consider the original score matching objective that minimizes the expected squared difference between the model’s score \\(s_\\theta(x)\\) (a neural network) and the true score \\(\\nabla_x \\log p(x)\\): . \\[\\mathcal{L} = \\mathbb{E}_{p(x)} \\left[ \\| s_\\theta(x) - \\nabla_x \\log p(x) \\|^2 \\right].\\] We have discussed that in the above. But since \\(\\nabla_x \\log p(x)\\) is unknown, this seems infeasible. We expand the loss: . \\[\\mathcal{L} = \\mathbb{E}_{p(x)} \\left[ | s_\\theta(x) |^2 - 2 s_\\theta(x)^T \\nabla_x \\log p(x) + | \\nabla_x \\log p(x) |^2 \\right]\\] then we split into three terms . \\[\\mathcal{L} = \\mathbb{E}_{p(x)} \\left[ | s_\\theta(x) |^2 \\right] - 2\\mathbb{E}_{p(x)} \\left[ s_\\theta(x)^T \\nabla_x \\log p(x) \\right] + \\mathbb{E}_{p(x)} \\left[ | \\nabla_x \\log p(x) |^2 \\right]\\] The third term is independent of \\(\\theta\\), so we leave it as-is and absorb it as constant. We look at the second term: . \\[\\mathbb{E}_{p(x)} \\left[ s_\\theta(x)^T \\nabla_x \\log p(x) \\right] = \\int s_\\theta(x)^T \\nabla_x \\log p(x) \\cdot p(x) dx= \\int s_\\theta(x)^T \\nabla_x p(x) dx,\\] since \\(\\nabla_x \\log p(x) = \\frac{\\nabla_x p(x)}{p(x)}\\). For each component \\(i\\), we now consider: . \\[\\int s_{\\theta,i}(x) \\frac{\\partial p(x)}{\\partial x_i} dx\\] Using integration by parts, with \\(u = s_{\\theta,i}(x)\\) and \\(dv = \\frac{\\partial p(x)}{\\partial x_i} dx\\), we have \\(du = \\frac{\\partial s_{\\theta,i}(x)}{\\partial x_i} dx\\) and \\(v = p(x)\\). The boundary term \\([s_{\\theta,i}(x) p(x)]\\) evaluated at the boundaries vanishes because \\(p(x) \\to 0\\) as \\(|x| \\to \\infty\\) (proper probability density), and \\(s_{\\theta,i}(x)\\) grows slower than \\(p(x)^{-1}\\) at infinity. Integration by parts results: . \\[\\int s_{\\theta,i}(x) \\frac{\\partial p(x)}{\\partial x_i} dx = -\\int \\frac{\\partial s_{\\theta,i}(x)}{\\partial x_i} p(x) dx\\] We then sum over all components: . \\[\\int s_\\theta(x)^T \\nabla_x p(x) dx = -\\int \\text{tr}(\\nabla_x s_\\theta(x)) p(x) dx\\] where \\(\\text{tr}(\\nabla_x s_\\theta(x)) = \\sum_i \\frac{\\partial s_{\\theta,i}(x)}{\\partial x_i}\\) is the divergence of \\(s_\\theta\\). Back to the expectation, we have now: . \\[\\mathbb{E}_{p(x)} \\left[ s_\\theta(x)^T \\nabla_x \\log p(x) \\right] = -\\mathbb{E}_{p(x)} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right]\\] Substitute to the original loss, we now have: . \\[\\begin{aligned} \\mathcal{L} &amp;= \\mathbb{E}_{p(x)} \\left[ \\| s_\\theta(x) \\|^2 \\right] - 2 \\left( -\\mathbb{E}_{p(x)} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right] \\right) + \\text{const} \\\\ &amp;= \\mathbb{E}_{p(x)} \\left[ \\| s_\\theta(x) \\|^2 \\right] + 2 \\mathbb{E}_{p(x)} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) \\right] + \\text{const} \\\\ \\implies \\mathcal{L} &amp;= \\mathbb{E}_{p(x)} \\left[ \\text{tr}(\\nabla_x s_\\theta(x)) + \\frac{1}{2} \\| s_\\theta(x) \\|^2 \\right] + \\text{const} \\end{aligned}\\] where the rescaling is okay for loss function as a minimization problem. The loss of this form is called Explicit Score Matching. It avoids dealing with true score , but computing the trace of the Jacobian is expensive for high-dimensional. We consider a noise-perturbed distribution that corrupts data with a known noise distribution (e.g., Gaussian): . \\[q_\\sigma(\\tilde{x}) = \\int q_{data}(x) q_\\sigma(\\tilde{x}|x) dx\\] where \\(q_\\sigma(\\tilde{x}|x)\\) is a Gaussian noise kernel: . \\[q_\\sigma(\\tilde{x}|x) = \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)\\] The key insight is that if we add noise to your data points, we can actually compute the score of the resulting distribution analytically because the noise is controlled. Thus we make the score estimation problem tractable where the noise structure allows you to derive a computable expression for the score function. The perturbation is designed to be with small enough \\(\\sigma\\). Starting with the original score matching objective but this time for \\(q_\\sigma(\\tilde{x})\\) instead of the original data distribution \\(q_{data}(x)\\): . \\[J_{SM}^{noisy}(\\theta) = \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} \\left[ \\frac{1}{2} |s_\\theta(\\tilde{x}) - \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})|^2 \\right]\\] Using the definition of \\(q_\\sigma(\\tilde{x}) = \\int q_{data}(x) q_\\sigma(\\tilde{x}|x) dx\\), we consider the weight in the expectation. Suppose there is an expectation of a random function \\(f(\\tilde{x})\\): . \\[\\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x})} [f(\\tilde{x})] = \\int f(\\tilde{x}) q_\\sigma(\\tilde{x}) d\\tilde{x}= \\int f(\\tilde{x}) \\left[\\int q_{data}(x) q_\\sigma(\\tilde{x}|x) dx\\right] d\\tilde{x}\\] By Fubini’s theorem (assuming integrability conditions are met), we continue having: \\(\\begin{align*} \\int f(\\tilde{x}) \\left[\\int q_{data}(x) q_\\sigma(\\tilde{x}|x) dx\\right] d\\tilde{x} &amp;= \\int q_{data}(x) \\left[\\int f(\\tilde{x}) q_\\sigma(\\tilde{x}|x) d\\tilde{x}\\right] dx \\\\ &amp;= \\int q_{data}(x) \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x}|x)} [f(\\tilde{x})] dx \\\\ &amp;= \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x}|x)} [f(\\tilde{x})] \\end{align*}\\) . Applying this amazing result to our score matching objective: . \\[J_{SM}^{noisy}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x}|x)} \\left[ \\frac{1}{2} |s_\\theta(\\tilde{x}) - \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})|^2 \\right]\\] The same-old problem of \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x})\\) is still there. Using the definition again: . \\[q_\\sigma(\\tilde{x}) = \\int q_{data}(x) q_\\sigma(\\tilde{x}|x) dx\\] %%take the gradient with respect to \\(\\tilde{x}\\): . \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) = \\frac{\\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x})}{q_\\sigma(\\tilde{x})}= \\frac{\\int q_{data}(x) \\nabla_{\\tilde{x}} q_\\sigma(\\tilde{x}|x) dx}{\\int q_{data}(x) q_\\sigma(\\tilde{x}|x) dx}\\] It’s not hard to imagine that when the noise kernel \\(q_\\sigma(\\tilde{x}|x)\\) is localized (e.g., Gaussian noise with small \\(\\sigma\\)), the dominant contribution to the integrals comes from \\(x\\) values close to \\(\\tilde{x}\\). So, for small noise levels, we can approximate: . \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}) \\approx \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}|x^*)\\] where \\(x^*\\) is the value of \\(x\\) that maximizes the posterior \\(q_{data}(x|\\tilde{x})\\), or more precisely, we’re using the fact that when we condition on a specific \\(x\\) in the inner expectation, the relevant score becomes \\(\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}|x)\\). Therefore, the objective becomes: . \\[J_{DSM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\tilde{x} \\sim q_\\sigma(\\tilde{x}|x)} \\left[ \\frac{1}{2} \\left|s_\\theta(\\tilde{x}) - \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}|x)\\right|^2 \\right]\\] For the noisy data \\(\\tilde{x} = x + \\sigma \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\), like we have done above in Tweedie’s formula deduction, we can compute the score analytically: . \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}|x) = \\nabla_{\\tilde{x}} \\log \\mathcal{N}(\\tilde{x}; x, \\sigma^2 I)= -\\frac{\\tilde{x} - x}{\\sigma^2}=-\\frac{\\epsilon}{\\sigma}\\] Then our analytical expression now becomes: . \\[J_{DSM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\frac{1}{2} \\left|s_\\theta(x + \\sigma\\epsilon) + \\frac{\\epsilon}{\\sigma}\\right|^2 \\right]\\] The true score is now gone. Here, the expectation over \\(\\epsilon\\) is analytically tractable because the noise \\(\\epsilon\\) is sampled from a simple Gaussian distribution and the term \\(\\frac{\\epsilon}{\\sigma}\\) is directly computable given \\(\\epsilon\\). Often, this is called Denoising Score Matching (DSM). Since the expectation is over a Gaussian distribution, we can efficiently estimate gradients using Monte Carlo sampling: \\(\\nabla_\\theta J_{DSM}(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\left( \\frac{1}{2} \\left\\| s_\\theta(x_i + \\sigma \\epsilon_i) + \\frac{\\epsilon_i}{\\sigma} \\right\\|^2 \\right),\\) where \\(x_i \\sim q_{data}(x)\\) and \\(\\epsilon_i \\sim \\mathcal{N}(0,I)\\). This is computationally efficient because sampling \\(\\epsilon_i\\) is cheap, and the loss is a simple \\(L_2\\) norm, making backpropagation straightforward. The DSM objective is equivalent to training: . \\[s_\\theta(x + \\sigma \\epsilon) \\approx -\\frac{\\epsilon}{\\sigma}.\\] This means the score network \\(s_\\theta\\) learns to predict the noise \\(\\epsilon\\) (scaled by \\(-\\frac{1}{\\sigma}\\)). (If pause for now, you’ll find the objective of what we are doing in this mathematical exegesis is literally narrowing down to formula shorter and shorter, while more and more clear). Then we can have: . \\[J_{DSM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\frac{1}{2\\sigma^2} |\\epsilon_\\theta(x + \\sigma\\epsilon) - \\epsilon(x)|^2 \\right]\\] This formulation directly predicts the noise that was added to the clean data. Another alternative to look at the loss is through clean data prediction \\(x_\\theta(\\tilde{x})\\) that predicts the clean data \\(x\\) from noisy data \\(\\tilde{x}\\). We define a new function from \\(\\tilde{x} = x + \\sigma \\epsilon\\): . \\[x_\\theta(\\tilde{x}) = \\tilde{x} - \\sigma \\epsilon(\\tilde{x})\\] where the \\(\\theta\\) denotes it being a function. By rearranging we have: . \\[\\epsilon(\\tilde{x}) = \\frac{\\tilde{x} - x_\\theta(\\tilde{x})}{\\sigma}\\] Substitute it in the original objective, we have: . \\[J_{DSM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\frac{1}{2\\sigma^2} \\left|\\frac{\\tilde{x} - x_\\theta(\\tilde{x})}{\\sigma} - \\epsilon\\right|^2 \\right]\\] Since \\(\\tilde{x} = x + \\sigma\\epsilon\\), we have \\(\\epsilon = \\frac{\\tilde{x} - x}{\\sigma}\\), and: . \\[\\frac{\\tilde{x} - x_\\theta(\\tilde{x})}{\\sigma} - \\epsilon = \\frac{\\tilde{x} - x_\\theta(\\tilde{x})}{\\sigma} - \\frac{\\tilde{x} - x}{\\sigma}\\] \\[= \\frac{\\tilde{x} - x_\\theta(\\tilde{x}) - \\tilde{x} + x}{\\sigma} = \\frac{x - x_\\theta(\\tilde{x})}{\\sigma}\\] Substitute back into the objective, we have the objective in new form: . \\[J_{DSM}(\\theta) = \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\frac{1}{2\\sigma^2} \\cdot \\frac{|x - x_\\theta(\\tilde{x})|^2}{\\sigma^2} \\right]\\] \\[= \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\frac{1}{2\\sigma^4} |x_\\theta(\\tilde{x}) - x|^2 \\right]\\] Here, we’re calm to see that to find the right score is the same thing as to denoise a sample, which matches the intuition. We keep fiddling with the formula. Given \\(\\tilde{x} = x + \\sigma \\epsilon\\) with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\), we have in effect a conditional distribution . \\[\\tilde{x} | x \\sim \\mathcal{N}(x,\\sigma^2I)\\] Therefore, we’re actually having: . \\[\\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}=\\mathbb{E}_{(x,\\tilde{x})\\sim q(x,\\tilde{x})}=\\mathbb{E}_{\\tilde{x}\\sim q(\\tilde{x})}\\mathbb{E}_{x\\sim q(x|\\tilde{x})}\\] with the joint distribution: . \\[q(x,\\tilde{x})=q_\\text{data}(x)\\cdot \\epsilon(\\tilde{x}|x)=q_\\text{data}(x)\\cdot \\mathcal{N}(\\tilde{x}|x,\\sigma^2I)\\] Then, firstly, from perspective of score prediction, by playing notation game on joint distribution, we have: . \\[\\begin{aligned} J_{DSM}(\\theta) &amp;= \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[\\frac{1}{2\\sigma^2} |\\epsilon(x + \\sigma\\epsilon) - \\epsilon|^2 \\right]\\\\ &amp;= \\mathbb{E}_{(x,\\tilde{x})\\sim q(x,\\tilde{x})}[\\frac{1}{2\\sigma^2}|\\epsilon(\\tilde{x}) - \\frac{\\tilde{x} - x}{\\sigma} |^2] \\\\ &amp;= \\mathbb{E}_{\\tilde{x}\\sim q(\\tilde{x})}\\mathbb{E}_{x\\sim q(x | \\tilde{x})}[\\frac{1}{2\\sigma^2}|\\epsilon(\\tilde{x}) - \\frac{\\tilde{x} - x}{\\sigma} |^2] \\\\ &amp;= \\mathbb{E}_{\\tilde{x}\\sim q(\\tilde{x})}[\\frac{1}{2\\sigma^2}|\\epsilon(\\tilde{x}) - \\frac{\\tilde{x} - \\mathbb{E}_{x\\sim q(x|\\tilde{x})}}{\\sigma} |^2] \\\\ \\end{aligned}\\] You may look at the last line and find that aside from its definition being a distribution about the difference between the sampled and true data, \\(\\epsilon\\) is learning the average noise direction that points to the given sample. And secondly, from perspective of sampling, we have: . \\[\\begin{aligned} J_{DSM}(\\theta) &amp;= \\mathbb{E}_{x \\sim q_{data}(x)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)} \\left[ \\frac{1}{2\\sigma^4} |x_\\theta(\\tilde{x}) - x|^2 \\right]\\\\ &amp;= \\mathbb{E}_{\\tilde{x}\\sim q(\\tilde{x})}\\mathbb{E}_{x\\sim q(x | \\tilde{x})} \\left[ \\frac{1}{2\\sigma^4} |\\tilde{x} + \\sigma^2\\cdot s_\\theta(\\tilde{x}) - x|^2 \\right] \\\\ &amp;= \\mathbb{E}_{\\tilde{x}\\sim q(\\tilde{x})} \\left[ \\frac{1}{2\\sigma^4} |\\tilde{x} + \\sigma^2\\cdot s_\\theta(\\tilde{x}) - \\mathbb{E}_{x\\sim q(x|\\tilde{x})}[x]|^2 \\right] \\\\ \\end{aligned}\\] And this echoes Tweedie’s Formula. I believe it is about time to close this long chapter. Now let’s turn our attention to diffusion model architecture again. Be it VAE (Variational Autoencoders), GAN or NF (Normalizing Flows), their generative process in sampling is a density transformation of this form . \\[x=f_\\theta(z),\\text{ where }z\\sim \\mathcal{N}(0,I).\\] as their backbone. The way they differ is mostly how they are trained. For VAEs, the transformation is straightforward - a single-step application of the decoder network \\(\\text{Decoder}_\\theta(z)\\) that maps from the latent space to the data space. GANs follow the exact same pattern in its \\(\\text{Generator}_\\theta(z)\\). For normalizing flows, the transformation is a composition of invertible transformations. Diffusion models have a more complex iterative structure: . \\[x = g_1(g_2(g_3(\\cdots g_T(z, s_\\theta), s_\\theta), s_\\theta), s_\\theta), \\text{ where } z \\sim \\mathcal{N}(0, I)\\] More precisely, this is often written as: . \\[x_0 = x_T, \\quad x_{t-1} = g(x_t, t, s_\\theta(x_t, t)) \\text{ for } t = T, T-1, \\ldots, 1\\] where \\(g(\\cdot)\\) represents the denoising step (which might include the reparameterization and sampling) and \\(s_\\theta\\) is the noise prediction network. Take a step back, you will see that the connection between climbing probability peaks and concrete training data is indeed fascinating - it happens through the clever design of the denoising objective. During training, we take real data samples (images, text, etc.) and systematically corrupt them by adding noise at different levels. This creates pairs of noisy sample, original sample at various noise levels. The model learns to predict what the original looked like given the noisy version. What’s brilliant is that this denoising process implicitly teaches the model about probability gradients. When you’re denoising, you’re essentially asking “which direction should I move in data space to make this sample more likely?” The answer points toward regions of higher probability density - exactly the direction you need to climb those peaks. This is supported by Tweedie’s formula, which equates denoising to finding the gradient of the log-probability density, i.e. score. And we deploy the equivalence of Fisher’s divergence with KL divergence to say, if you find out a good score, then you’re actually building a good model that is close to the true distribution. So the concrete training process - showing the model millions of noisy/clean pairs - gradually builds up an implicit map of where the “peaks” are in data space. The model never explicitly computes probabilities, but through denoising it learns the topography of the probability landscape. During generation, following the learned denoising steps traces paths from random noise toward these learned peaks of real data. It’s like learning to navigate a mountain range by repeatedly practicing how to step uphill from any given position, without ever seeing the full map. In this chapter, we introduce ELBO to maximize the observed data’s probability so that the distribution closest to the true distribution can be found, which is the foundational principle of variational inference. Then, by introducing multiple latents , we can pick encoder as gaussian noise adding so that central limit theorem guarantees the final latent being gaussian, which facilitates sampling so well by its rich features. However, we have to link decoder network training with our goal of modeling the best distribution. Tweedie’s formula dictates that denoising is equivalent to moving in the right score direction (the magnitude does not so matter because of the concentrated shell phenomenon in high dimension). The equivalence of Fisher divergence vs. KL divergence as well as the use of Fokker-Planck equation tells us the finding the right score direction during sampling is in fact finding the distribution for the samples close to the true distribution. So the problem has become to train the score. We finally discussed some score matching, giving us insights to our work. On closing this chapter, we have found ourselves understanding the basic spirit of diffusion model with consistent logic chain and concrete mathematic deduction. Up till now, you should be able to, with the several deus ex machina tricks, deduce the score matching formulae from scratch with a pencil and a sheet of paper. If you cannot do that, please review the three chapters; else, congrats, and you move on to Chapter 4. ",
    "url": "/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/03-chpt3.html",
    
    "relUrl": "/03-chpt3.html"
  },"4": {
    "doc": "Chapter 4 - Implementation on machine - get our hands dirty",
    "title": "Chapter 4 - Implementation on machine - get our hands dirty",
    "content": "Chapter 4 - Implementation on machine - get our hands dirty. Implementation is one of the most beautiful thing that you have experienced on earth, because it is the last push from idea on your mind to reality. In Chapter 3, we came to know that above all, the training is noise prediction, as is demonstrated in the discussion of equivalent loss functions. Chapter 3 also provides mathematical tools to push forward the realization of the thinking in this chapter. We will offer the specific forward distribution transition formula, i.e. noise-adding procedure/forward process, as well as the reverse distribution transition formula, i.e. the denoising procedure/reverse process. This chapter is aimed to push mathematics to executable level. We start from . \\[\\mu(x,t) = -\\frac{1}{2}\\beta(t),\\quad \\sigma(x,t) = \\sqrt{\\beta}\\] as solution to our Itô equation. That they push the initial distribution towards converging gaussian is validated in Chapter 3. The discrete update rules led by that provides more control for algorithm, e.g. accelerate and de-accelerate in noise schedule. Start with the SDE: . \\[dx_t = -\\frac{1}{2} \\beta(t) x_t \\, dt + \\sqrt{\\beta(t)} \\, dW_t\\] For a small time step \\(\\Delta t = 1\\), the discretization gives: . \\[x_t \\approx x_{t-1} - \\frac{1}{2} \\beta(t) x_{t-1} + \\sqrt{\\beta(t)} \\, (W_t - W_{t-1})\\] Since \\(W_t - W_{t-1} \\sim \\mathcal{N}(0, \\Delta t = 1)\\), let \\(z_{t-1} = W_t - W_{t-1} \\sim \\mathcal{N}(0, I)\\): . \\[x_t \\approx \\left(1 - \\frac{1}{2} \\beta(t)\\right) x_{t-1} + \\sqrt{\\beta(t)} \\, z_{t-1}\\] For small \\(\\beta(t)\\), take the first-order Taylor expansion of \\(\\sqrt{1 - \\beta(t)}\\): . \\[\\sqrt{1 - \\beta(t)} \\approx 1 - \\frac{1}{2} \\beta(t)\\] Substitute this into above, we then have: . \\(x_t = \\sqrt{1 - \\beta(t)} \\, x_{t-1} + \\sqrt{\\beta(t)} \\, z_{t-1}\\) as the forward process. As we recall the reverse SDE in Chapter 3 that claims . \\[d\\mathbf{x} = \\left[ \\mathbf{\\mu}(\\mathbf{x}, t) - \\sigma(t)^2 \\nabla_{\\mathbf{x}} \\log p_t(\\mathbf{x}) \\right] dt + \\sigma(t) d\\bar{\\mathbf{w}}_t\\] We put our solution of \\(\\mu(x,t)\\) and \\(\\sigma(x,t)\\) in it, and have: . \\[d\\mathbf{x} = \\left[ -\\frac{1}{2}\\beta(t) - \\beta(t) \\nabla_{\\mathbf{x}} \\log p_t(\\mathbf{x}) \\right] dt + \\sigma(t) d\\bar{\\mathbf{w}}_t\\] where the \\(p_t(\\mathbf{x})\\) is similar to the noise-perturbed distribution that corrupts data with a known noise distribution (e.g., Gaussian) in Chapter 3: . \\[q_\\sigma(\\tilde{x}) = \\int q_{data}(x) q_\\sigma(\\tilde{x}|x) dx\\] We should establish the expression of \\(x_t\\) on \\(x_0\\), because the latter matches \\(q_{data}(x)\\) in essence. However, we only have the expression of \\(x_t\\) on \\(x_{t-1}\\): . \\[x_t = \\sqrt{1 - \\beta_t} \\, x_{t-1} + \\sqrt{\\beta_t} \\, \\varepsilon_{t-1}, \\quad \\varepsilon_{t-1} \\sim \\mathcal{N}(0, I),\\] where \\(\\beta_t\\) is the noise schedule. So we accumulate that till \\(t=0\\), then we have . \\[x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon,\\] where \\(\\alpha_t = 1 - \\beta_t\\), \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\), and \\(\\varepsilon \\sim \\mathcal{N}(0, I)\\). We can also rewrite it as . \\[q(x_t | x_0) = \\mathcal{N}\\left(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I \\right),\\] where \\(\\alpha_t = 1 - \\beta_t\\), and \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\). Make analogy to our analysis to . \\[\\tilde{x} = x + \\sigma \\epsilon\\] where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) in Chapter 3, according to . \\[x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon,\\] similar to . \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}|x) = - \\frac{\\varepsilon}{\\sigma}\\] there, we have . \\[\\nabla_{x_t} \\log p_t(x_t) \\approx -\\frac{\\epsilon_\\theta(x_t, t)}{\\sqrt{1 - \\bar{\\alpha}_t}}.\\] However, this analogy is not convincing after all. Let’s pause a little bit and discuss more on this. First, let’s restate the two scenarios, first being . \\[\\tilde{x} = x + \\sigma \\epsilon\\] and second being: . \\[x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\varepsilon\\] In the first scenario, it does no harm to assume \\( \\tilde{x} = x + \\sigma \\varepsilon \\), then: . \\[\\tilde{x} | x \\sim \\mathcal{N}(x, \\sigma^2)\\] The probability density function is: . \\[q_\\sigma(\\tilde{x} | x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left( -\\frac{(\\tilde{x} - x)^2}{2\\sigma^2} \\right)\\] Taking the logarithm: . \\[\\log q_\\sigma(\\tilde{x} | x) = -\\frac{(\\tilde{x} - x)^2}{2\\sigma^2} - \\log(\\sqrt{2\\pi}\\sigma)\\] Now, taking the gradient with respect to \\( \\tilde{x} \\): . \\[\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x} | x) = -\\frac{2(\\tilde{x} - x)}{2\\sigma^2} = -\\frac{\\tilde{x} - x}{\\sigma^2}\\] Now, we are so certain that taking the gradient in the second scenario would definitely be . \\[\\nabla_{\\tilde{x}} \\log q(\\tilde{x} | x) = -\\frac{2(\\tilde{x} - \\sqrt{\\bar{\\alpha}} \\, x)}{2(1 - \\bar{\\alpha})} = -\\frac{\\tilde{x} - \\sqrt{\\bar{\\alpha}} \\, x}{1 - \\bar{\\alpha}} = -\\frac{\\varepsilon}{\\sqrt{1 - \\bar{\\alpha}}}\\] by solid analogy. Ok, now we go back to our case by substituting the score approximation: . \\[dx_t = \\left[ -\\frac{1}{2} \\beta(t) x_t + \\frac{\\beta(t)}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right] dt + \\sqrt{\\beta(t)} \\, d\\bar{W}_t.\\] For small steps (\\(\\Delta t = 1\\)), the reverse update becomes: . \\[x_{t-1} = x_t - \\left[ -\\frac{1}{2} \\beta_t x_t + \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right] + \\sqrt{\\beta_t} \\, z,\\] where \\(z \\sim \\mathcal{N}(0, I)\\). Simplifying it, we have: . \\[x_{t-1} = \\left(1 + \\frac{\\beta_t}{2}\\right) x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) + \\sqrt{\\beta_t} \\, z.\\] By far, all basic theoretical knowledge for diffusion models is illustrated. Among most of the diffusion models, all variants use the same loss . \\[\\mathbb{E}[|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)|^2].\\] All methods implicitly use Tweedie’s formula to estimate the clean data \\(\\hat{\\mathbf{x}}_0\\) from the noisy observation \\(\\mathbf{x}_t\\), where the relationship . \\[\\boldsymbol{\\epsilon}_\\theta = -\\sqrt{1-\\bar{\\alpha}_t} \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)\\] connects denoising to score matching. Below, we will discuss the real code repository for the famous Latent Diffusion Model. Our discussion will include model architecture and its core building blocks, the underlying design rationale and key implementation details, and the connections to other leading diffusion-based approaches, etc. This chapter will remain open-ended by design, allowing for future expansions as new advancements emerge in this rapidly evolving field. It’ll keep growing, fo’ sho’. Let’s implement! . ",
    "url": "/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/04-chpt4.html",
    
    "relUrl": "/04-chpt4.html"
  },"5": {
    "doc": "Cover Page",
    "title": "Cover Page",
    "content": "A Preliminary Mathematical Exegesis of Diffusion Model -- gaining clarity from true understanding . A PrgM2 (/pRˈɡem/2)'s work The author reserves all right. No copy without the author's consent. No re-distribution without the author's consent. Let this thing just be minor contribution to our culture, a'ight? be cool. [init]2025/06/16. Created vibe with jeykll and just-the-doc. Uploaded chapters 0, 1, and 2. [grow]2025/06/18. Uploaded Chapter 3. Fixed some typos. [grow]2025/06/18. Added aiding split line throught the chapters. Fixed some typos. [TBD] - latent diffuison model illustration - conditional generation SOTA - Mamba + diffusion ",
    "url": "/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/",
    
    "relUrl": "/"
  },"6": {
    "doc": "About Author",
    "title": "About Author",
    "content": "Shuyue Wang, the author. Working on Artificial Intelligence. Currently living at Shanghai. Please send messages to him via: henri_w_91@hotmail.com . ",
    "url": "/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/resume.html",
    
    "relUrl": "/resume.html"
  },"7": {
    "doc": "Buy me coffee(s) / become sponsor",
    "title": "Buy me coffee(s) / become sponsor",
    "content": "Please buy me coffee(s) or be my sponsor. Use Alipay to scan this: . Sponsor list (top 20 in sponsorship): . | Emzan Technology Ltd., Hong Kong SAR, China. | . ",
    "url": "/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/sponsor.html",
    
    "relUrl": "/sponsor.html"
  }
}
