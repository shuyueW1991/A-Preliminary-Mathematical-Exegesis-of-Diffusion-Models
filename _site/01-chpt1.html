<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(3)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(3) > a { font-weight: 600; text-decoration: none; } .site-nav > ul.nav-category-list > li > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(3) > button svg { transform: rotate(-90deg); } .site-nav > ul.nav-category-list > li.nav-list-item > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(3) > ul.nav-list { display: block; } </style> <script src="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Chapter 1 - The High-Dimensional Structure of True Data | PrgM2 /pR&#39;gem/2</title> <meta name="generator" content="Jekyll v3.10.0" /> <meta property="og:title" content="Chapter 1 - The High-Dimensional Structure of True Data" /> <meta property="og:locale" content="en_US" /> <link rel="canonical" href="http://localhost:4000/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/01-chpt1.html" /> <meta property="og:url" content="http://localhost:4000/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/01-chpt1.html" /> <meta property="og:site_name" content="PrgM2 /pR&#39;gem/2" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Chapter 1 - The High-Dimensional Structure of True Data" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","headline":"Chapter 1 - The High-Dimensional Structure of True Data","url":"http://localhost:4000/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/01-chpt1.html"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/" class="site-title lh-tight"> PrgM2 /pR\'gem/2 </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/" class="nav-list-link">Cover Page</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/00-chpt0.html" class="nav-list-link">Chapter 0 - Preface</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/01-chpt1.html" class="nav-list-link">Chapter 1 - The High-Dimensional Structure of True Data</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/sponsor.html" class="nav-list-link">Sponsors</a></li><li class="nav-list-item"><a href="/A-Preliminary-Mathematical-Exegesis-of-Diffusion-Models/resume.html" class="nav-list-link">About Author</a></li></ul> <div class="nav-category"></div> <ul class="nav-list"></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div></div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async=""></script> <div style="text-align: left; font-size: 1.3em;"> Chapter 1 - The High-Dimensional Structure of True Data </div> <p><br /></p> <div style="text-align: center;"> <img src="./assets/images/fig_ch01.png" style="width: 45%; max-width: 400px; height: auto; margin: 0 auto;" /> </div> <p>We kickstart our exegesis by reiterating the fundamental objective of diffusion models claimed in the preface—To replicate what the nature offers, mostly in image generation. Consider a photograph taken from Shangrī-La, a hand-drawn portrait in the <em>Musée du Louvre</em>, or plants documented in <em>Amazonía</em> —each constitutes a manifestation of nature’s visual repertoire. We formally designate such representations as natural images.</p> <p>It is not difficult to understand, though, that being ‘natural’ for images is not the same with ‘real’. A “real” image is bound by the constraints of physical reality, whereas a “natural” image may include artistic interpretations, stylized elements, or even idealized forms that still feel intuitively coherent. For instance, an impressionist painting or a dreamlike digital rendering can qualify as natural despite departing from strict realism. Diffusion models, therefore, aim not just for photorealism but for the broader fidelity to natural visual grammar—the interplay of light, form, and texture that defines recognizability. This subtle distinction underscores their flexibility in generating both lifelike and creatively altered outputs while remaining anchored in naturalistic principles.</p> <p>In each of the above visual case, the visual input can be represented as a tensor—for instance, a \(256 \times 256\) image admits ${2^8}^3 = 16,777,216$ possible colors per pixel (where 3 represents 3 RGB channels and 8 refers to the bit depth of the channel), and the total configuration space for pixel combinations therefore contains $(16,777,216)^{256 \times 256}$ distinct images—a vast number that exceeds the estimated $10^{80}$ atoms in the observable universe.</p> <p>If we randomly assign values to each pixel, the result will be more likely to appear chaotic than to be ‘natural’, almost certainly degrading into meaningless noise. This can be empirically verified by generating random arrays (e.g., via <code class="language-plaintext highlighter-rouge">numpy.random.rand()</code>in python) and visualizing them using any standard image display tool. Such a disordered array of uncorrelated hues would diverge sharply from the characteristics of a typical natural image. This is because natural images exhibit a structured harmony, where pixels organize into coherent objects, textures, and spatial relationships—manifesting as edges, discernible shapes, and smooth luminance gradients. While mathematically valid as tensors, these random permutations are exploiting the full combinatorial space without yielding meaningful representation. In fact, only a vanishingly small fraction of these possible configurations correspond to meaningful images.</p> <p>The sparsity of meaningful images might suggest Gaussian distributions as potential models, given their prevalence in other domains.<br /> However, Gaussian approximations fail dramatically for natural images.<br /> When fitted to image data through mean and covariance, sampling yields only blurry, unrealistic outputs if using Gaussian approximations. You can also try it by yourself. The mean can be an indistinct average (like an “average face”), while the covariance captures merely pairwise pixel relationships. This failure illustrates a deeper truth: semantics has finer structure than simple enumerations. These fundamental inadequacies necessitate more sophisticated approaches—such as diffusion models—capable of capturing the intricate structures inherent in natural images.</p> <p>Since arbitrary pixel arrangements rarely yield realistic outputs, we turn to probability theory for a solution, because real natural images follow certain patterns (e.g., smooth textures, recognizable objects), we can model distribution that registers those patterns statistically rather than relying on randomness. Here, each natural image can be viewed as a sampled event from an underlying probability distribution over all possible pixel configurations. In probability theory, an event represents a possible outcome of an experiment or observation.<br /> It is denoted by $x$, which may be either a scalar in one dimension or a tensor in multiple dimensions.<br /> When considering images, $x$ typically represents either meaningful natural images or random arrangements of pixels.<br /> The probability $p(x)$ then quantifies the likelihood of event $x$ occurring.<br /> It calculates the likelihood of observing a specific natural image among all possible pixel configurations. In this framework, every natural image thus represents a sample drawn from an underlying probability distribution governing all possible pixel configurations. A good $p(x)$ should approach zero for random noise while assigning higher probability density to natural images. With this foundation, our focus will now shift entirely to modeling the true data distribution $p_{data}(x)$, the underlying probability structure governing natural images. For practitioners seeking to leverage generative AI—whether to develop custom models or satisfy theoretical curiosity—understanding this distribution’s properties proves essential.</p> <p>Now let’s take a look at the high-dimensional tensor of noise with $d$ pixels, each being independent with identically distributed (i.e. i.i.d.) components.:</p> \[X=(X_1,X_2,…,X_d)\] <p>We suppose $X_i$ has finite variance $\sigma^2$. Since: \(∥X∥_2^2=\sum_{i=1}^dX_i^2\)we have \(\mathbb{E}[\|X\|_2^2]=d\cdot\mathbb{E}[X_i^2]\)and \(Var(\|X\|_2^2)=d⋅Var(X_i^2).\) We calculate the ratio between the standard deviation and the mean, and we refer to this ratio as concentration ratio: \(\frac{\sqrt{Var(∥X∥_2^2)}}{E[∥X∥_2^2]}=\frac{\sqrt{d⋅Var(X_i^2)}}{d⋅E[X_i^2]} \propto \frac{1}{\sqrt{d}}→0\quad\text{as }d \rightarrow \infty.\) This vanishing result implies the concentration around the mean, which you can imagine a thin shell of noises shows itself gathering the sample when the dimension increases. We now illustrate this concentrated norm phenomenon for high-dimensional noise in uniform and gaussian distributions, respectively —</p> <p>For $X_i \sim \text{Uniform}(-a,a)$，its probability density function is \(f_{X_i}(x) = \frac{1}{2a}.\)The variance is: \(\begin{align*} \text{Var}(X_i^2) &amp;= \mathbb{E}[X_i^4] - (\mathbb{E}[X_i^2])^2 \\ &amp;= \left( \int_{-a}^a x^4 \cdot \frac{1}{2a} \, dx \right) - \left( \int_{-a}^a x^2 \cdot \frac{1}{2a} \, dx \right)^2 \\ &amp;= \left( \frac{1}{2a} \left[ \frac{x^5}{5} \right]_{-a}^a \right) - \left( \frac{1}{2a} \left[ \frac{x^3}{3} \right]_{-a}^a \right)^2 \\ &amp;= \frac{a^4}{5} - \left( \frac{a^2}{3} \right)^2 \\ &amp;= \frac{4a^4}{45} \end{align*}\) For $X = (X_1, \dots, X_d)$, we have: \(\mathbb{E}[\|X\|_2^2] = d \cdot \mathbb{E}[X_i^2] = d \cdot \frac{a^2}{3},\) so \(\|X\|_2 \approx \sqrt{\mathbb{E}[\|X\|_2^2]} = a \sqrt{\frac{d}{3}}.\) Also, we have: \(\text{Var}(\|X\|_2^2) = d \cdot \text{Var}(X_i^2) = d \cdot \frac{4a^4}{45},\) so \(\sigma_{\|X\|_2^2} = \frac{2a^2 \sqrt{d}}{\sqrt{45}} = \frac{2a^2 \sqrt{d}}{3 \sqrt{5}}.\) Then we now have the concentration ratio vanishing in the uniform distribution: \(\frac{\sigma_{\|X\|_2^2}}{\mathbb{E}[\|X\|_2^2]} = \frac{2a^2 \sqrt{d} / (3 \sqrt{5})}{d a^2 / 3} = \frac{2}{\sqrt{5d}}\to 0\text{ as }d \to \infty.\)</p> <p>For $X_i \sim \mathcal{N}(0, \sigma^2)$, its probability density function is: \(f_{X_i}(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{x^2}{2\sigma^2}}.\) The variance of $X_i^2$: \(\begin{align*} \text{Var}(X_i^2) &amp;= \mathbb{E}[X_i^4] - \left(\mathbb{E}[X_i^2]\right)^2 \\ &amp;= \left( \int_{-\infty}^{\infty} x^4 f_{X_i}(x) \, dx \right) - \left( \int_{-\infty}^{\infty} x^2 f_{X_i}(x) \, dx \right)^2 \\ &amp;= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^{\infty} x^4 e^{-\frac{x^2}{2\sigma^2}} \, dx \right) - \left( \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^{\infty} x^2 e^{-\frac{x^2}{2\sigma^2}} \, dx \right)^2 \\ &amp;= \left( \frac{1}{\sqrt{2\pi\sigma^2}} \cdot 3\sqrt{2\pi}\sigma^5 \right) - \left( \frac{1}{\sqrt{2\pi\sigma^2}} \cdot \sqrt{2\pi}\sigma^3 \right)^2 \\ &amp;= 3\sigma^4 - (\sigma^2)^2 \\ &amp;= 2\sigma^4 \end{align*}\) For $X = (X_1, \dots, X_d)$, we have: \(\mathbb{E}[\|X\|_2^2] = d \cdot \mathbb{E}[X_i^2] = d \sigma^2,\) so \(\|X\|_2 \approx \sqrt{\mathbb{E}[\|X\|_2^2]} = \sigma \sqrt{d}.\) And we have \(\text{Var}(\|X\|_2^2) = d \cdot \text{Var}(X_i^2) = 2d \sigma^4,\) so \(\sigma_{\|X\|_2^2} = \sigma^2 \sqrt{2d}.\) Then the concentration ratio in the gaussian distribution becomes:\(\frac{\sigma_{\|X\|_2^2}}{\mathbb{E}[\|X\|_2^2]} = \frac{\sigma^2 \sqrt{2d}}{d \sigma^2} = \sqrt{\frac{2}{d}} \to 0 \text{ as } d \to \infty.\)</p> <p>Now let’s check upon the relationship between the noises inside the shell. Consider two random noises $\mathbf{u}, \mathbf{v} \in \mathbb{R}^d$ , with i.i.d. entries , each has finite variances and bounded fourth moments: \(\text{Var}(u_i) = \sigma_u^2, \text{Var}(v_i) = \sigma_v^2 &lt; \infty,\text{and } \mathbb{E}[u_i^4], \mathbb{E}[v_i^4] &lt; \infty\) The dot product is \(\langle \mathbf{u}, \mathbf{v} \rangle \triangleq \sum_{i=1}^d u_i v_i\) has expectation : \(\mathbb{E}[\langle \mathbf{u}, \mathbf{v} \rangle] = \sum^d_{i=1}\mathbb{E}[u_iv_i] = \sum^d_{i=1}\mathbb{E}[u_i]\mathbb{E}[v_i] = d \mu_u \mu_v.\) The split of expectation comes from i.i.d. features. Its variance is: \(\text{Var}(u_i v_i) = \mathbb{E}[u_i^2 v_i^2] - (\mathbb{E}[u_i v_i])^2 = (\sigma_u^2 + \mu_u^2)(\sigma_v^2 + \mu_v^2) - \mu_u^2 \mu_v^2=\sigma^2​_u\sigma_v^2​+\sigma^2​_v\mu_u^2​+\sigma^2​_u\mu_v^2​.\)</p> <p>Here comes the first <em>deux ex machina</em> of the booklet:</p> <blockquote> <div class="table-wrapper"><table> <tbody> <tr> <td>The Chebyshev inequality provides a bound on the probability that a random variable $X$ deviates from its mean $\mu$ by more than $k$ standard deviations $\sigma$, stating that $\Pr(</td> <td>X - \mu</td> <td>\geq k\sigma) \leq \frac{1}{k^2}$ for any $k &gt; 0$.</td> </tr> </tbody> </table></div> </blockquote> <p>This universal bound applies to any distribution with finite variance, offering a measure of dispersion guarantees. Back to our case, for $\epsilon &gt; 0$: \(\mathbb{P}\left( \left| \frac{\langle \mathbf{u}, \mathbf{v} \rangle}{d} - \mu_u \mu_v \right| \geq \epsilon \right) \leq \frac{\text{Var}(\langle \mathbf{u}, \mathbf{v} \rangle)}{d^2 \epsilon^2} = \frac{d\cdot (\sigma^2​_u\sigma_v^2​+\sigma^2​_v\mu_u^2​+\sigma^2​_u\mu_v^2)}{d^2 \epsilon^2} \to 0 \text{ as } d \to \infty.\) Thus, equivalently: \(\frac{\langle \mathbf{u}, \mathbf{v} \rangle}{d} \to \mu_u \mu_v.\)</p> <p>The squared norm of $\mathbf{u}$ is:\(\|\mathbf{u}\|^2 = \sum_{i=1}^d u_i^2.\) So, compute the expected value: \(\mathbb{E}\left[\frac{\|\mathbf{u}\|^2}{d}\right] = \frac{1}{d} \sum_{i=1}^d \mathbb{E}[u_i^2].\)Since \(\mathbb{E}[u_i^2] = \text{Var}(u_i) + (\mathbb{E}[u_i])^2 = \sigma_u^2 + \mu_u^2,\)we have:\(\mathbb{E}\left[\frac{\|\mathbf{u}\|^2}{d}\right] =\frac{d\cdot(\sigma_u^2 + \mu_u^2)}{d}= \sigma_u^2 + \mu_u^2.\) Then, similar to the use of the Chebyshev inequality, the squared norms concentrate as: \(\frac{\|\mathbf{u}\|^2}{d} \to \sigma_u^2 + \mu_u^2, \quad \frac{\|\mathbf{v}\|^2}{d} \to \sigma_v^2 + \mu_v^2.\)</p> <p>The normalized dot product thus satisfies: \(\frac{\langle \mathbf{u}, \mathbf{v} \rangle}{\|\mathbf{u}\| \|\mathbf{v}\|} = \frac{\langle \mathbf{u}, \mathbf{v} \rangle / d}{\sqrt{(\|\mathbf{u}\|^2 / d)(\|\mathbf{v}\|^2 / d)}} \to \frac{\mu_u \mu_v}{\sqrt{(\sigma_u^2 + \mu_u^2)(\sigma_v^2 + \mu_v^2)}}.\) From the definniton of inner vectorial product, if $\mu_u \mu_v = 0$ (i.e. at least one mean is zero), there is orthogonality between noises within the shell. That is to say, for zero-mean random vectors in high dimensions, if the distribution to sample is zero-meaned, the sampled vectors are likely to be orthogonal to each other. Under this circumstance, the directions of noise vectors remain uniformly distributed over the sphere, which is quite unusual in low-dimensional space.</p> <p>For a more intuitive grasp of high-dimensional noise , let’s check how a perturbation added to a high-dimensional tensor would be like. Consider a random noise vector $\boldsymbol{\epsilon} \in \mathbb{R}^d$, where each component $\epsilon_i$ is sampled i.i.d. from a distribution with mean $0$ and variance $\sigma^2$. By Chebyshev’s inequality, the normalized squared norm $|\boldsymbol{\epsilon}|_2^2/d$ concentrates around $\sigma^2$: \(\Pr\left(\left|\frac{\|\boldsymbol{\epsilon}\|_2^2}{d} - \sigma^2\right| \geq \delta\right) \leq \frac{\text{Var}(\epsilon_i^2)}{d\delta^2},\) implying $|\boldsymbol{\epsilon}|_2 \approx \sigma\sqrt{d}$ for large $d$. It means that even small noise ($\sigma$) becomes significant in high dimensions, causing large prediction changes. For example, a linear model $f(\mathbf{x}) = \mathbf{w}^T \mathbf{x}$, the perturbation’s impact scales as $|\mathbf{w}^T \boldsymbol{\epsilon}| \leq |\mathbf{w}|_2 \sigma \sqrt{d}$. In the context of images, small perturbations to pixel values can induce large changes in model predictions. For a given portrait, it takes not much of effort to make it not portrait-like any more by perturbing the pixels.</p> <p>The quasi-orthogonality of high-dimensional noise vectors and the concentration of their norms play crucial roles in diffusion models. These properties ensure efficient exploration of the data space during the forward process and stable reconstruction during denoising. Like a well-calibrated search algorithm, they enable the model to systematically perturb and smear entirely the high-dimensional space. We will analyze these in detail in later chapters.</p> <p>Our analysis of high-dimensional data naturally leads to the question: what distinguishes natural images within this space that is mostly occupied by noise? We take a look at their norms, first. Consider a $32 \times 32$ grayscale image, representing a vector in $\mathbb{R}^{1024}$ space ($d = 32 \times 32 = 1024$). Let $\mathbf{x} \in \mathbb{R}^d$ denote the image and $\boldsymbol{\epsilon} \in \mathbb{R}^d$ represent standard normal noise ($\epsilon_i \sim \mathcal{N}(0, 1)$). The expected squared $L_2$ norm of the noise is: \(\mathbb{E}[\|\boldsymbol{\epsilon}\|_2^2] = \sum^d_{i}\mathbb{E}[\|\epsilon_i\|_2^2] = d \cdot \sigma^2 = 1024 \cdot 1 = 1024\) which yields a typical noise magnitude of $|\boldsymbol{\epsilon}|_2 \approx \sqrt{1024} = 32$. In contrast, studies have found that the average squared $L2$​ norm of CIFAR-10 images (after normalization) is roughly 200, yielding a typical norm of $\sqrt{200}\sim14.14$, significantly smaller than that of a random noise. We find that natural images exhibit smaller norms compared to random noise with independent and identically distributed (i.i.d.) entries.</p> <p>Three key factors explain this phenomenon mathematically. First, natural images exhibit piecewise smoothness: they consist of regions with slowly varying pixel values (like smooth surfaces or uniform areas) separated by abrupt transitions (edges). Since edges occupy relatively few pixels, their contribution to the overall norm is minimal compared to smooth regions. Second, the presence of sharp edges creates heavy-tailed distributions. Unlike Gaussian distributions, natural images show higher probability mass in extreme values due to frequent abrupt transitions between regions. Third, the abundance of near-zero pixel values (from dark areas or low-intensity regions) further reduces the overall norm. Approximately 50% of pixels in natural images often fall into this category. This norm discrepancy demonstrates that natural images cannot be modeled as random high-dimensional vectors with i.i.d. components. Correlations indicated by smooth edges, textures, and objects imply that the data is not filling the high-dimensional space uniformly but instead concentrates near a lower-dimensional structure.</p> <p>Another perspective to understand this is through entropy. Entropy measures the average uncertainty (or information content) of event $X$, where higher entropy means more randomness that makes compression harder, and lower entropy means more predictability that makes allowing better compression. Regarding flipping a coin, a fair coin (50% heads, 50% tails) should have maximum entropy because you’re completely uncertain about the outcome, while a biased coin (e.g., 99% heads) should have lower entropy, because you’re more confident it’ll land heads. A higher entropy should bring more surprise.</p> <p>In this spirit, Shannon’s entropy is thus defined: for a discrete random variable $X$ with possible outcomes $x_i$ and probabilities $p_i$​: \(H(X)=− \sum_{i} ​p_i(X)​ \log p_i(X)\) For an image $X$ with pixel  $x_i\in{0,1,…,255}$ as channel value, the entropy per pixel is: \(H_{noise}=-\sum_{i=0}^{255}p(x_i)\log_2p(x_i)\) We consider random noise as uniformly distributed, then its entropy is \(-\sum_{i=0}^{255}\frac{1}{256}\log_2\frac{1}{256}=\log⁡_2 256=8 \text{bits/pixel}.\)</p> <p>Knowing this, we now look at the joint entropy $H(X,Y)$ that measures the total uncertainty of the pair $(X,Y)$: \(H(X,Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P(x,y) \log_2 P(x,y),\) where $P(x,y)$ is the joint probability of $X = x$ and $Y = y$. When $X$ and $Y$ are independent, the joint probability factorizes: \(P(x,y) = P(x)P(y).\) Substituting into the joint entropy formula: \(H(X,Y) = -\sum_{x} \sum_{y} P(x)P(y) \log_2 \left[ P(x)P(y) \right]= -\sum_{x} \sum_{y} P(x)P(y) \left[ \log_2 P(x) + \log_2 P(y) \right].\) Splitting the sum into:\(-\sum_{x} P(x) \log_2 P(x) \sum_{y} P(y) = H(X)\)and \(-\sum_{y} P(y) \log_2 P(y) \sum_{x} P(x) = H(Y).\) Thus: \(H(X,Y) = H(X) + H(Y).\) So, for an $n\times n$ image with $n^2$ pixels, if pixel values are independent, then the total entropy is additive:\(H_{total}=n^2\times H=n^2\times 8 \text{bits}.\)</p> <p>Natural images have much lower entropy than random noise images, meaning they occupy an exponentially smaller fraction of the space of all possible pixel combinations. This is also concluded from empirical studies. Due to correlations, natural images have much lower entropy. An image of a face is determined by lighting, pose, and identity, not pixel-by-pixel independently. Therefore, due to strong statistical dependencies, their effective entropy is significantly lower than their nominal dimensionality would suggest. This reduction in entropy implies that natural images possess far fewer degrees of freedom than independent pixel sampling would permit. Natural images do not fill their high-dimensional pixel space uniformly; instead, they reside in an exponentially sparse subset.</p> <p>On one hand, natural images are highly structured, more complex than a random noise. And they have smaller norms compared with a random noise. On the other hand, we have known that natural images are vanishingly rare in the space of all possible pixel combinations. Randomly sampling from all possible $256\times256$ RGB images gives you almost 0 probability of getting a natural image.   But if you sample from real-world camera outputs, the 0 probability goes to the meaningless noise, It does no harm to imagine that maybe, natural images dominate the probability mass of the true data distribution $p_{data}(x)$ i.e. it is highly peaked around natural images.</p> <p>Thus, while natural images may be rare in the global view of high-dimensional space, they dominate within certain localized regions. This structure is formally referred to as a manifold ($\mathcal{M}$) in the machine learning community—a low-dimensional geometric object embedded within a high-dimensional space. A useful analogy is a two-dimensional sheet of paper floating in three-dimensional space. Globally, the paper occupies no volume—it is infinitely thin—yet every point on its surface lies precisely within its own two-dimensional plane. Similarly, natural images, though sparse in the full pixel space, concentrate on much lower-dimensional manifolds where their intrinsic structure resides.</p> <p>Mathematically speaking, in the space of all possible images, the set of natural images has extremely small volume but very high density $p_{data}(x)$ for $x\in M$. The rest of the space (noise, random pixels) has enormous volume but near-zero density $p_{data}(x)\approx 0$. Sampling from $p_{data}$​ thus almost always gives natural images. When generating new images, there might be chances of wandering into low-probability regions. However, if your model $p(x)$ is good enough, sampling from it correctly will almost always give natural images, because in a well-trained p(x), almost all the probability mass is concentrated around the natural image manifold.</p> <p>Note that Probability mass ≠ size of set: This is the key insight! The size of a set (e.g., natural images vs. noise) is not the same as its probability mass. Probability mass depends on the distribution $p(x)$. If you randomly sample pixels (uniformly), you’ll almost always get noisy garbage. but if you cast a regard to real world, you rarely see random noise. Therefore, a well-learned generative model should resemble this true data distribution where the <strong>high-density regions</strong> should correspond to natural images, and the <strong>low-density regions</strong> should correspond to noise or unrealistic images. They shrink the sampling space to near $\mathcal{M}$, making natural images likely outputs.</p> <p>If we would like to realize such a modeled distribution, we can use a suitable model family chosen as $p_\theta(x)$, with unknown parameters $\theta$, so that the problem boils down to maximizing the average (log-)likelihood (w.r.t $\theta$) of all the samples under the model \(\theta^* = \arg \max_\theta \mathbb{E}_{x \sim q_{\text{data}}(x)} [\log p_\theta(x)] \approx \arg \max_\theta \frac{1}{N} \sum_{n=1}^N \log p_\theta(x^{(n)})\) where defining an arbitrary parametric density $p_\theta(x)$ is not as easy as it looks.</p> <p>There was one aspect of $p_\theta$ that is widely considered to be the evil behind this difficulty – the normalizing constant that stems from the axiom of probability \(p_\theta(x) = \frac{\tilde{p}_\theta(x)}{\int_x \tilde{p}_\theta(x)}\) A smart solution to this would be brought up in chapters to follow.</p> <p>In summary, in the space of all pixel combination, even though natural images are a <em>tiny subset</em> of all possible images, they carry <em>almost all</em> the probability mass in $p(x)$. This is the benefit of getting a modeled distribution that is close to true distribution. Equivalently, the closer a modeled distribution to the true distribution, the more probability of observed data getting sampled.</p> <p>The problem of generative modeling can be posed as parametric density estimation using a finite set of observed samples ${x^{(n)}}^N_{n=1}$ from a true but unknown data distribution $p_{true}(x)$. The observed data $x$ are observed because there is a true distribution out there, and the data point represented by the observed data occupies most of the probability mass of the true distribution. There should be a value measuring the probability of observing those $x$.</p> </main> </div> </div> </div> </body> </html>
